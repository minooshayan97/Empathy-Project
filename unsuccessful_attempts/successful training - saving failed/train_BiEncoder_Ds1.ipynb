{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-25T00:39:18.320104Z","iopub.status.busy":"2024-10-25T00:39:18.319121Z","iopub.status.idle":"2024-10-25T00:39:31.821148Z","shell.execute_reply":"2024-10-25T00:39:31.820026Z","shell.execute_reply.started":"2024-10-25T00:39:18.320043Z"},"executionInfo":{"elapsed":13665,"status":"ok","timestamp":1729811889433,"user":{"displayName":"Minoo Shayan","userId":"09011516747950461668"},"user_tz":420},"id":"aP-2A1gCkqsq","outputId":"0d6244aa-e3e2-4355-ca79-a7932f87f75f","trusted":true},"outputs":[],"source":["!pip install -q transformers datasets wandb"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-25T00:39:31.824505Z","iopub.status.busy":"2024-10-25T00:39:31.823712Z","iopub.status.idle":"2024-10-25T00:39:33.487470Z","shell.execute_reply":"2024-10-25T00:39:33.486251Z","shell.execute_reply.started":"2024-10-25T00:39:31.824457Z"},"executionInfo":{"elapsed":3499,"status":"ok","timestamp":1729811898145,"user":{"displayName":"Minoo Shayan","userId":"09011516747950461668"},"user_tz":420},"id":"0VA7Mpf1ktNB","outputId":"ce56f53a-8e22-4ba5-b482-cc1751bbcecf","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token hf_"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-25T00:39:33.489825Z","iopub.status.busy":"2024-10-25T00:39:33.489433Z","iopub.status.idle":"2024-10-25T00:45:06.748842Z","shell.execute_reply":"2024-10-25T00:45:06.748094Z","shell.execute_reply.started":"2024-10-25T00:39:33.489790Z"},"id":"tsK_q1ackMS0","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61da96f07dab45e4af06d495dea95298","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113737922222653, max=1.0‚Ä¶"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241025_004454-6iwv7kfz</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-empathy/runs/6iwv7kfz' target=\"_blank\">tough-sunset-7</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-empathy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-empathy' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-empathy/runs/6iwv7kfz' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy/runs/6iwv7kfz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f3d975d6c204c8a947ac2b6902afc0b","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/589 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77fcc8b1618b4129bde62adf04a9a3e7","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e86dd53ab4fa431bb79005167b007dc7","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/118k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12832f15727c4b3ab0175d5598d12af9","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/118k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a861bbfab0c4aa6a42605d99c92ce98","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/643 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d34a70caeb744678ba3faf0e5237db27","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3f7ff98dd644b06ab9d6aa41a4942d4","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/81 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fba47ff373949dfb9eb1a91c460872b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc88f7d429e043589d4511af5ce5b1aa","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b33649389fb48c28830c7add1a17f2f","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfe71748f1ef488daa9ff5cdf59ecb3f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59dbf376ca3a4310bc95ba53371e1bf2","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1789fe1468fd47258420019a81e4dbd0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/643 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fcc72f00df94343ad2a27e2c5eb8deb","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4a64d38de0e4169a57b0c9ee356a08a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/81 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n","from transformers import BertConfig, BertModel\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.stats import pearsonr, spearmanr\n","import wandb\n","import numpy as np\n","\n","# Initialize wandb\n","wandb.init(\n","    project=\"bert-biencoder-regression\"\n",")\n","\n","# Load dataset\n","dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n","\n","# Initialize bi-encoder model (e.g., BERT as a sentence encoder)\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","base_model = AutoModel.from_pretrained(model_name)\n","\n","# Tokenize both text1 and text2 independently\n","def preprocess_function(examples):\n","    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n","    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n","    return {\n","        'input_ids_text1': text1_encodings['input_ids'],\n","        'attention_mask_text1': text1_encodings['attention_mask'],\n","        'input_ids_text2': text2_encodings['input_ids'],\n","        'attention_mask_text2': text2_encodings['attention_mask'],\n","        'labels': examples['label']\n","    }\n","\n","# Apply tokenization\n","tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n","\n","# Remove unnecessary columns and set format for PyTorch\n","columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']\n","tokenized_train.set_format(type='torch', columns=columns_to_keep)\n","tokenized_test.set_format(type='torch', columns=columns_to_keep)\n","tokenized_val.set_format(type='torch', columns=columns_to_keep)\n","\n","# Define a custom collator to handle text1 and text2 encoding\n","class BiEncoderCollator:\n","    def __call__(self, features):\n","        batch = {\n","            'input_ids_text1': torch.stack([f['input_ids_text1'] for f in features]),\n","            'attention_mask_text1': torch.stack([f['attention_mask_text1'] for f in features]),\n","            'input_ids_text2': torch.stack([f['input_ids_text2'] for f in features]),\n","            'attention_mask_text2': torch.stack([f['attention_mask_text2'] for f in features]),\n","            'labels': torch.tensor([f['labels'] for f in features], dtype=torch.float)\n","        }\n","        return batch\n","\n","collator = BiEncoderCollator()\n","\n","# Define the compute_metrics function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.squeeze()\n","    labels = labels.squeeze()\n","\n","    mse = mean_squared_error(labels, predictions)\n","    mae = mean_absolute_error(labels, predictions)\n","    pearson_corr, _ = pearsonr(predictions, labels)\n","    spearman_corr, _ = spearmanr(predictions, labels)\n","    cosine_sim = torch.nn.functional.cosine_similarity(torch.tensor(predictions), torch.tensor(labels), dim=0).mean().item()\n","\n","    return {\n","        \"mse\": mse,\n","        \"mae\": mae,\n","        \"pearson_corr\": pearson_corr,\n","        \"spearman_corr\": spearman_corr,\n","        \"cosine_sim\": cosine_sim  # Optional metric for similarity tasks\n","    }\n","\n","# Define a custom BiEncoder model\n","class BiEncoderModel(torch.nn.Module):\n","    def __init__(self, base_model, config=None, loss_fn=\"mse\"):\n","        super(BiEncoderModel, self).__init__()\n","        self.base_model = base_model\n","        self.cos = torch.nn.CosineSimilarity(dim=1)\n","        self.loss_fn = loss_fn\n","        self.config = config\n","\n","    def forward(self, input_ids_text1, attention_mask_text1, input_ids_text2, attention_mask_text2, labels=None):\n","        # Encode text1 and text2 separately\n","        outputs_text1 = self.base_model(input_ids_text1, attention_mask=attention_mask_text1)\n","        outputs_text2 = self.base_model(input_ids_text2, attention_mask=attention_mask_text2)\n","\n","        # Extract [CLS] token embeddings (first token)\n","        cls_embedding_text1 = outputs_text1.last_hidden_state[:, 0, :]\n","        cls_embedding_text2 = outputs_text2.last_hidden_state[:, 0, :]\n","\n","        # Calculate cosine similarity between the two embeddings\n","        cos_sim = self.cos(cls_embedding_text1, cls_embedding_text2)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.loss_fn == \"mse\":\n","                loss_fct = torch.nn.MSELoss()  # Mean Squared Error Loss\n","            elif self.loss_fn == \"mae\":\n","                loss_fct = torch.nn.L1Loss()  # Mean Absolute Error Loss\n","            elif self.loss_fn == \"contrastive\":\n","                loss_fct = self.contrastive_loss\n","            elif self.loss_fn == \"cosine_embedding\":\n","                loss_fct = torch.nn.CosineEmbeddingLoss()  # Cosine Embedding Loss\n","\n","            if self.loss_fn == \"cosine_embedding\":\n","                labels_cosine = 2 * (labels > 0.5).float() - 1  # Convert labels to binary for cosine embedding loss\n","                loss = loss_fct(cls_embedding_text1, cls_embedding_text2, labels_cosine)\n","            else:\n","                loss = loss_fct(cos_sim, labels)\n","\n","        return {\"loss\": loss, \"logits\": cos_sim}\n","\n","    def contrastive_loss(self, cos_sim, labels, margin=0.5):\n","        loss = torch.mean((1 - labels) * torch.pow(cos_sim, 2) + labels * torch.pow(torch.clamp(margin - cos_sim, min=0.0), 2))\n","        return loss\n","\n","# Initialize the Bi-Encoder model with a specific loss function\n","def train_biencoder(loss_fn):\n","    # Load pre-trained BERT configuration and model\n","    config = BertConfig.from_pretrained(model_name)\n","    bert_model = BertModel.from_pretrained(model_name)\n","\n","    # Initialize your custom BiEncoderModel with the BERT model and config\n","    bi_encoder_model = BiEncoderModel(base_model=bert_model, config=config, loss_fn=loss_fn)\n","    #bi_encoder_model = BiEncoderModel(base_model, loss_fn)\n","\n","    # Define TrainingArguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"./output/empathy-biencoder-{loss_fn}\",\n","        evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n","        logging_dir='./logs',           # Directory for logs\n","        logging_steps=10,               # Log every 10 steps\n","        per_device_train_batch_size=wandb.config['batch_size'],\n","        per_device_eval_batch_size=wandb.config['batch_size'],\n","        num_train_epochs=wandb.config['epochs'],\n","        warmup_steps=100,\n","        learning_rate=wandb.config['learning_rate'],\n","        weight_decay=0.01,\n","        report_to=\"wandb\",\n","        save_strategy=\"epoch\",          # Save checkpoints at the end of each epoch\n","        load_best_model_at_end=True,\n","        push_to_hub=True,\n","        save_total_limit=2              # Keep only the 2 most recent checkpoints\n","    )\n","\n","    # Define the Trainer\n","    trainer = Trainer(\n","        model=bi_encoder_model,             # Custom BiEncoder model\n","        args=training_args,                 # Training arguments\n","        train_dataset=tokenized_train,      # Training dataset\n","        eval_dataset=tokenized_val,         # Validation dataset\n","        data_collator=collator,             # Custom collator for handling bi-encoder inputs\n","        compute_metrics=compute_metrics     # Function to compute metrics\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    # Evaluate the model on the test set\n","    #trainer.evaluate(tokenized_test)\n","\n","    trainer.model = trainer.model.model\n","\n","    # Save the model to Hugging Face Hub\n","    trainer.save_model(f\"./output/bert-reg-biencoder-{loss_fn}\")\n","    trainer.push_to_hub(f\"minoosh/bert-reg-biencoder-{loss_fn}\")\n","\n","    # Finish wandb run\n","    wandb.finish()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hKzcAt8eNLsa","outputId":"a1fadc49-f90c-44c5-ef49-1c280e5c616f","trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:szv1l4p9) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">stilted-microwave-1</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-empathy/runs/szv1l4p9' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy/runs/szv1l4p9</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-empathy' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241024_235738-szv1l4p9/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:szv1l4p9). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241024_235749-q9hhvp83</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-empathy/runs/q9hhvp83' target=\"_blank\">bert-biencoder-empathy-cosine_embedding</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-empathy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-empathy' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-empathy/runs/q9hhvp83' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy/runs/q9hhvp83</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [147/147 07:51, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","      <th>Mae</th>\n","      <th>Pearson Corr</th>\n","      <th>Spearman Corr</th>\n","      <th>Cosine Sim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.536600</td>\n","      <td>0.521217</td>\n","      <td>0.136733</td>\n","      <td>0.291817</td>\n","      <td>0.107817</td>\n","      <td>0.104922</td>\n","      <td>0.904680</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.529400</td>\n","      <td>0.506096</td>\n","      <td>0.099628</td>\n","      <td>0.239027</td>\n","      <td>0.138539</td>\n","      <td>0.151546</td>\n","      <td>0.904774</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.481100</td>\n","      <td>0.486177</td>\n","      <td>0.085464</td>\n","      <td>0.238144</td>\n","      <td>0.132545</td>\n","      <td>0.089930</td>\n","      <td>0.885686</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.429200</td>\n","      <td>0.449410</td>\n","      <td>0.151136</td>\n","      <td>0.320174</td>\n","      <td>0.215457</td>\n","      <td>0.177835</td>\n","      <td>0.787157</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.367200</td>\n","      <td>0.451264</td>\n","      <td>0.147009</td>\n","      <td>0.306734</td>\n","      <td>0.220627</td>\n","      <td>0.184861</td>\n","      <td>0.797954</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.326000</td>\n","      <td>0.464310</td>\n","      <td>0.220934</td>\n","      <td>0.390014</td>\n","      <td>0.186157</td>\n","      <td>0.166267</td>\n","      <td>0.669728</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.286600</td>\n","      <td>0.461217</td>\n","      <td>0.214077</td>\n","      <td>0.370943</td>\n","      <td>0.180855</td>\n","      <td>0.162266</td>\n","      <td>0.697175</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.031 MB of 0.031 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>‚ñà‚ñà‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÑ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>eval/mae</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñá‚ñÜ</td></tr><tr><td>eval/mse</td><td>‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÜ</td></tr><tr><td>eval/pearson_corr</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ</td></tr><tr><td>eval/spearman_corr</td><td>‚ñÇ‚ñÜ‚ñÅ‚ñá‚ñà‚ñá‚ñÜ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>0.76741</td></tr><tr><td>eval/loss</td><td>0.4725</td></tr><tr><td>eval/mae</td><td>0.34612</td></tr><tr><td>eval/mse</td><td>0.18548</td></tr><tr><td>eval/pearson_corr</td><td>0.20373</td></tr><tr><td>eval/runtime</td><td>2.5697</td></tr><tr><td>eval/samples_per_second</td><td>31.132</td></tr><tr><td>eval/spearman_corr</td><td>0.18348</td></tr><tr><td>eval/steps_per_second</td><td>1.167</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/grad_norm</td><td>3.43398</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2866</td></tr><tr><td>train_loss</td><td>0.41864</td></tr><tr><td>train_runtime</td><td>475.6733</td></tr><tr><td>train_samples_per_second</td><td>9.462</td></tr><tr><td>train_steps_per_second</td><td>0.309</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-biencoder-empathy-cosine_embedding</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-empathy/runs/q9hhvp83' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy/runs/q9hhvp83</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-empathy' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-empathy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241024_235749-q9hhvp83/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Train bi-encoder with different loss functions\n","loss_functions = [\"mse\", \"mae\", \"contrastive\", \"cosine_embedding\"]\n","loss_fn = loss_functions[3]\n","wandb.init(project=\"bert-biencoder-empathy\", name=f\"bert-biencoder-empathy-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","train_biencoder(loss_fn)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"103de50b6741efac643968c186d1b1abfce3e31cc37d5e54d8bda505b68efb83"}}},"nbformat":4,"nbformat_minor":4}
