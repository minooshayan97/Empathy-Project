{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-30T22:08:55.309771Z","iopub.status.busy":"2024-10-30T22:08:55.309489Z","iopub.status.idle":"2024-10-30T22:09:07.951366Z","shell.execute_reply":"2024-10-30T22:09:07.949914Z","shell.execute_reply.started":"2024-10-30T22:08:55.309740Z"},"trusted":true},"outputs":[],"source":["!pip install -q transformers datasets wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T22:09:07.953669Z","iopub.status.busy":"2024-10-30T22:09:07.953321Z","iopub.status.idle":"2024-10-30T22:09:09.542210Z","shell.execute_reply":"2024-10-30T22:09:09.541355Z","shell.execute_reply.started":"2024-10-30T22:09:07.953629Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token hf_"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T22:09:09.544029Z","iopub.status.busy":"2024-10-30T22:09:09.543707Z","iopub.status.idle":"2024-10-30T22:10:37.499057Z","shell.execute_reply":"2024-10-30T22:10:37.498201Z","shell.execute_reply.started":"2024-10-30T22:09:09.543993Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0cc8bc0fcab4415bdd22543f44a8911","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113508911110632, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241030_221029-mskhcamp</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/mskhcamp' target=\"_blank\">spectral-crypt-5</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/mskhcamp' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/mskhcamp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77e1f07b5fe44cec878f211d360e8f4a","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/588 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd1a0497da0849c5831759baf5f10a2b","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/660k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36000e4d6d754ee38dbd39ed003dcfd9","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/100k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"382c55d31d7349c788815ad6b2dafab8","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/88.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2e4d1406c6640078e266d535cf87b80","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/2467 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c674ddb14a664c30951e21bb31d81be5","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2026c32114ff4f77953666ff7fbc44dd","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/309 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23c0c415951146288c6057dd9a8e2ad0","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc74b16662bb473fb0c172e0b3c6a098","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42b82e643edd485289ef9b7704faed6b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"603a0e675468494d9a39898b5aee5c50","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4f46ca4800240a5ab0db2c90a9480ca","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2467 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"148260be86344988995379a8189054e3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/308 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1a5778e463b4c7c97485e8e5273de3a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/309 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import wandb\n","\n","# Initialize wandb\n","wandb.init(\n","    project=\"bert-crossencoder-classification\"\n",")\n","\n","# Load dataset\n","dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n","\n","# Initialize the tokenizer and model for cross-encoder setup\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Preprocess data for the cross-encoder model by concatenating text1 and text2 with [SEP]\n","def preprocess_function(examples):\n","    # Concatenate both texts with a [SEP] token in between\n","    encodings = tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n","    encodings['labels'] = examples['label']  # Add labels\n","    return encodings\n","\n","# Apply tokenization\n","tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n","\n","# Set format for PyTorch\n","tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# Define compute_metrics function for classification evaluation\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    preds = predictions.argmax(axis=1)\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average=\"weighted\")\n","    recall = recall_score(labels, preds, average=\"weighted\")\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1\n","    }\n","\n","# Custom Cross-Encoder model class for classification\n","class CrossEncoderModel(torch.nn.Module):\n","    def __init__(self, model_name, num_classes=4, loss_fn=\"cross_entropy\"):\n","        super(CrossEncoderModel, self).__init__()\n","        # Load model config\n","        self.config = AutoConfig.from_pretrained(model_name, num_labels=num_classes)\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=self.config)\n","        self.loss_fn = loss_fn\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Output logits for classification\n","\n","        loss = None\n","        if labels is not None:\n","            if self.loss_fn == \"cross_entropy\":\n","                loss_fct = torch.nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n","                loss = loss_fct(logits, labels)\n","            elif self.loss_fn == \"focal_loss\":\n","                # Focal loss implementation for handling class imbalance\n","                alpha = 0.25\n","                gamma = 2.0\n","                ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, labels)\n","                pt = torch.exp(-ce_loss)  # Probability of the true class\n","                loss = (alpha * (1 - pt) ** gamma * ce_loss).mean()\n","            elif self.loss_fn == \"kl_divergence\":\n","                # KL Divergence for soft-label classification\n","                kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","                soft_labels = torch.nn.functional.one_hot(labels, num_classes=self.config.num_labels).float()\n","                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","                loss = kl_div(log_probs, soft_labels)\n","            else:\n","                raise ValueError(f\"Unsupported loss function: {self.loss_fn}\")\n","\n","        return {\"loss\": loss, \"logits\": logits}\n","\n","# Function to initialize and train the cross-encoder model\n","def train_crossencoder(loss_fn):\n","    model = CrossEncoderModel(model_name=model_name, loss_fn=loss_fn)\n","\n","    # Set up TrainingArguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"./output/empathy-crossencoder-{loss_fn}\",\n","        evaluation_strategy=\"epoch\",\n","        logging_dir='./logs',\n","        logging_steps=10,\n","        per_device_train_batch_size=wandb.config['batch_size'],\n","        per_device_eval_batch_size=wandb.config['batch_size'],\n","        num_train_epochs=wandb.config['epochs'],\n","        warmup_steps=100,\n","        learning_rate=wandb.config['learning_rate'],\n","        weight_decay=0.01,\n","        report_to=\"wandb\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        push_to_hub=True,\n","        save_total_limit=2\n","    )\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_val,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    # Evaluate the model on the test set\n","    trainer.evaluate(tokenized_test)\n","\n","    # Save and push the model to the Hugging Face Hub\n","    trainer.save_model(f\"./output/empathy-crossencoder-{loss_fn}\")\n","    trainer.push_to_hub(f\"minoosh/empathy-crossencoder-{loss_fn}\")\n","\n","    # End the wandb run\n","    wandb.finish()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T21:30:52.671355Z","iopub.status.busy":"2024-10-30T21:30:52.671063Z","iopub.status.idle":"2024-10-30T21:46:02.501200Z","shell.execute_reply":"2024-10-30T21:46:02.500470Z","shell.execute_reply.started":"2024-10-30T21:30:52.671323Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:l5bgt6y2) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">muted-beast-1</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/l5bgt6y2' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/l5bgt6y2</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241030_213044-l5bgt6y2/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:l5bgt6y2). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241030_213052-ei4d1y9k</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/ei4d1y9k' target=\"_blank\">bert-crossencoder-classification-cross_entropy</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/ei4d1y9k' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/ei4d1y9k</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a13f62c9fdb468d88a05e5f346f3374","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [546/546 14:36, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.234600</td>\n","      <td>1.167360</td>\n","      <td>0.469256</td>\n","      <td>0.280394</td>\n","      <td>0.469256</td>\n","      <td>0.346311</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.993000</td>\n","      <td>1.034654</td>\n","      <td>0.576052</td>\n","      <td>0.611152</td>\n","      <td>0.576052</td>\n","      <td>0.544453</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.799700</td>\n","      <td>0.931385</td>\n","      <td>0.605178</td>\n","      <td>0.611788</td>\n","      <td>0.605178</td>\n","      <td>0.607487</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.698400</td>\n","      <td>0.971698</td>\n","      <td>0.608414</td>\n","      <td>0.616197</td>\n","      <td>0.608414</td>\n","      <td>0.608083</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.442700</td>\n","      <td>1.036381</td>\n","      <td>0.611650</td>\n","      <td>0.613078</td>\n","      <td>0.611650</td>\n","      <td>0.610664</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.389200</td>\n","      <td>1.054925</td>\n","      <td>0.595469</td>\n","      <td>0.603016</td>\n","      <td>0.595469</td>\n","      <td>0.598054</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.276700</td>\n","      <td>1.097122</td>\n","      <td>0.576052</td>\n","      <td>0.585239</td>\n","      <td>0.576052</td>\n","      <td>0.578687</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","HTTP Error 500 thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/2c/f7/2cf7041ff51d6d12d44fcc53fb3ebaedfd0100dc90d8d5f9ea9912f5907cbe63/00951279a1a5fd43a7393ff4aaf6cabe926e1f835bb6baa0217d1f499f05b98a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241030T214533Z&X-Amz-Expires=86400&X-Amz-Signature=5dd1322a705894487e3ca2ab6e464efb8db11ff70e7e55bca133fe8d7ac754c3&X-Amz-SignedHeaders=host&partNumber=19&uploadId=1LOnOb9uIt4tb7XwNxGNvybLZIsPa.b5YP2FDy4QZX8.s7vQj0AUla0FvClPOlzwatYdRtQWzt6umTxwyzjjpAK24lab.XVWI7pn_DbDJ4SMr0dvYKnDlPXFuTmSCTH_&x-id=UploadPart\n","Retrying in 1s [Retry 1/5].\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10/10 00:04]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.032 MB of 0.032 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆███▇▆▇</td></tr><tr><td>eval/f1</td><td>▁▆████▇▇</td></tr><tr><td>eval/loss</td><td>█▄▁▂▄▅▆▃</td></tr><tr><td>eval/precision</td><td>▁█████▇▇</td></tr><tr><td>eval/recall</td><td>▁▆███▇▆▇</td></tr><tr><td>eval/runtime</td><td>▁▅▇█▇█▇▅</td></tr><tr><td>eval/samples_per_second</td><td>█▄▂▁▂▁▂▄</td></tr><tr><td>eval/steps_per_second</td><td>█▄▂▁▂▁▂▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▆▂▁▁▂▁▁▃▃▃▂▂▄▄▃▄▅▅▃▄▃▄█▇▅▃▇▄▅▆▄▅▃▆▄▅▄▃▃</td></tr><tr><td>train/learning_rate</td><td>▂▂▃▄▄▆▇▇███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>████▇▇▇▇▆▆▆▅▅▅▅▄▅▄▄▃▄▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.58442</td></tr><tr><td>eval/f1</td><td>0.58415</td></tr><tr><td>eval/loss</td><td>0.98529</td></tr><tr><td>eval/precision</td><td>0.58427</td></tr><tr><td>eval/recall</td><td>0.58442</td></tr><tr><td>eval/runtime</td><td>4.9666</td></tr><tr><td>eval/samples_per_second</td><td>62.014</td></tr><tr><td>eval/steps_per_second</td><td>2.013</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>6.73136</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2767</td></tr><tr><td>train_loss</td><td>0.74804</td></tr><tr><td>train_runtime</td><td>879.6794</td></tr><tr><td>train_samples_per_second</td><td>19.631</td></tr><tr><td>train_steps_per_second</td><td>0.621</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-crossencoder-classification-cross_entropy</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/ei4d1y9k' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/ei4d1y9k</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241030_213052-ei4d1y9k/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Specify list of loss functions to try\n","loss_functions = [\"cross_entropy\", \"focal_loss\", \"kl_divergence\"]\n","\n","loss_fn = loss_functions[0]  # Change to desired loss function\n","wandb.init(project=\"bert-crossencoder-classification\", name=f\"bert-crossencoder-classification-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","train_crossencoder(loss_fn)\n","wandb.finish()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T21:47:45.983724Z","iopub.status.busy":"2024-10-30T21:47:45.983274Z","iopub.status.idle":"2024-10-30T22:03:07.807398Z","shell.execute_reply":"2024-10-30T22:03:07.806569Z","shell.execute_reply.started":"2024-10-30T21:47:45.983658Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:zisgk0li) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">shadowed-beast-3</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/zisgk0li' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/zisgk0li</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241030_214738-zisgk0li/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:zisgk0li). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241030_214746-3gjc047u</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/3gjc047u' target=\"_blank\">bert-crossencoder-classification-focal_loss</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/3gjc047u' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/3gjc047u</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65dad4080a644905a122d2409a1d5281","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [546/546 14:46, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.169700</td>\n","      <td>0.168782</td>\n","      <td>0.459547</td>\n","      <td>0.457934</td>\n","      <td>0.459547</td>\n","      <td>0.431939</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.131800</td>\n","      <td>0.132317</td>\n","      <td>0.576052</td>\n","      <td>0.615534</td>\n","      <td>0.576052</td>\n","      <td>0.553035</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.110000</td>\n","      <td>0.118426</td>\n","      <td>0.579288</td>\n","      <td>0.581347</td>\n","      <td>0.579288</td>\n","      <td>0.576467</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.087600</td>\n","      <td>0.119102</td>\n","      <td>0.595469</td>\n","      <td>0.608861</td>\n","      <td>0.595469</td>\n","      <td>0.598462</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.058900</td>\n","      <td>0.122868</td>\n","      <td>0.611650</td>\n","      <td>0.613869</td>\n","      <td>0.611650</td>\n","      <td>0.611476</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.046200</td>\n","      <td>0.126162</td>\n","      <td>0.605178</td>\n","      <td>0.619178</td>\n","      <td>0.605178</td>\n","      <td>0.608539</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.033700</td>\n","      <td>0.130708</td>\n","      <td>0.595469</td>\n","      <td>0.609251</td>\n","      <td>0.595469</td>\n","      <td>0.600171</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10/10 00:04]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.031 MB of 0.031 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▇▇██▇▇</td></tr><tr><td>eval/f1</td><td>▁▆▇▇███▇</td></tr><tr><td>eval/loss</td><td>█▃▁▁▂▂▃▂</td></tr><tr><td>eval/precision</td><td>▁█▆████▇</td></tr><tr><td>eval/recall</td><td>▁▆▇▇██▇▇</td></tr><tr><td>eval/runtime</td><td>█▃▂▃▃▂▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇▇▇▇▄█</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▇▆▇▇▄█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄█▃▁▂▂▄▁▂▄▂▁▂▄▃▄▅▃▂▃▂▂▅▃▂▃▃▄▂▁▅▃▃▄▅▂▁▂▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▂▃▄▄▆▇▇███▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.60065</td></tr><tr><td>eval/f1</td><td>0.58985</td></tr><tr><td>eval/loss</td><td>0.1266</td></tr><tr><td>eval/precision</td><td>0.59672</td></tr><tr><td>eval/recall</td><td>0.60065</td></tr><tr><td>eval/runtime</td><td>5.0018</td></tr><tr><td>eval/samples_per_second</td><td>61.578</td></tr><tr><td>eval/steps_per_second</td><td>1.999</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>0.74752</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0337</td></tr><tr><td>train_loss</td><td>0.09918</td></tr><tr><td>train_runtime</td><td>889.7358</td></tr><tr><td>train_samples_per_second</td><td>19.409</td></tr><tr><td>train_steps_per_second</td><td>0.614</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-crossencoder-classification-focal_loss</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/3gjc047u' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/3gjc047u</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241030_214746-3gjc047u/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Specify list of loss functions to try\n","loss_functions = [\"cross_entropy\", \"focal_loss\", \"kl_divergence\"]\n","\n","loss_fn = loss_functions[1]  # Change to desired loss function\n","wandb.init(project=\"bert-crossencoder-classification\", name=f\"bert-crossencoder-classification-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","train_crossencoder(loss_fn)\n","wandb.finish()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T22:10:37.500974Z","iopub.status.busy":"2024-10-30T22:10:37.500666Z","iopub.status.idle":"2024-10-30T22:27:05.689192Z","shell.execute_reply":"2024-10-30T22:27:05.688344Z","shell.execute_reply.started":"2024-10-30T22:10:37.500941Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:mskhcamp) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">spectral-crypt-5</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/mskhcamp' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/mskhcamp</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241030_221029-mskhcamp/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:mskhcamp). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241030_221037-p0v493tx</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/p0v493tx' target=\"_blank\">bert-crossencoder-classification-kl_divergence</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/p0v493tx' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/p0v493tx</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8075ed2365904bf8ba7e95323d4233c2","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [546/546 15:53, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.226300</td>\n","      <td>1.177230</td>\n","      <td>0.527508</td>\n","      <td>0.559952</td>\n","      <td>0.527508</td>\n","      <td>0.506894</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.013400</td>\n","      <td>1.025421</td>\n","      <td>0.585761</td>\n","      <td>0.611601</td>\n","      <td>0.585761</td>\n","      <td>0.556446</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.836200</td>\n","      <td>0.962687</td>\n","      <td>0.611650</td>\n","      <td>0.641748</td>\n","      <td>0.611650</td>\n","      <td>0.613409</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.709800</td>\n","      <td>0.949051</td>\n","      <td>0.618123</td>\n","      <td>0.637895</td>\n","      <td>0.618123</td>\n","      <td>0.622453</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.550400</td>\n","      <td>0.953130</td>\n","      <td>0.621359</td>\n","      <td>0.626690</td>\n","      <td>0.621359</td>\n","      <td>0.623061</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.448900</td>\n","      <td>0.989292</td>\n","      <td>0.608414</td>\n","      <td>0.616237</td>\n","      <td>0.608414</td>\n","      <td>0.610485</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.340500</td>\n","      <td>0.998353</td>\n","      <td>0.614887</td>\n","      <td>0.621817</td>\n","      <td>0.614887</td>\n","      <td>0.617107</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10/10 00:04]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.031 MB of 0.031 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▇██▇██</td></tr><tr><td>eval/f1</td><td>▁▄▇██▇██</td></tr><tr><td>eval/loss</td><td>█▃▁▁▁▂▃▂</td></tr><tr><td>eval/precision</td><td>▁▅██▇▆▆▆</td></tr><tr><td>eval/recall</td><td>▁▅▇██▇██</td></tr><tr><td>eval/runtime</td><td>█▄▄▄▄▄▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▅▅▅▅▅▆█</td></tr><tr><td>eval/steps_per_second</td><td>▁▅▅▅▅▅▆█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▁▆▂▂▁▃▄▃▁▃▃▃▂▂▄▄▄▅▆▃▅▄▄▄▇▅▅▆▅█▃▅▆▅█▅▄▇█▄</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▄▆▇███▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▇▇▇▆▆▅▅▅▅▅▄▄▄▄▃▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.61688</td></tr><tr><td>eval/f1</td><td>0.61596</td></tr><tr><td>eval/loss</td><td>0.97487</td></tr><tr><td>eval/precision</td><td>0.62385</td></tr><tr><td>eval/recall</td><td>0.61688</td></tr><tr><td>eval/runtime</td><td>5.4699</td></tr><tr><td>eval/samples_per_second</td><td>56.308</td></tr><tr><td>eval/steps_per_second</td><td>1.828</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>7.05949</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3405</td></tr><tr><td>train_loss</td><td>0.77626</td></tr><tr><td>train_runtime</td><td>956.3594</td></tr><tr><td>train_samples_per_second</td><td>18.057</td></tr><tr><td>train_steps_per_second</td><td>0.571</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-crossencoder-classification-kl_divergence</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/p0v493tx' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/p0v493tx</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241030_221037-p0v493tx/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Specify list of loss functions to try\n","loss_functions = [\"cross_entropy\", \"focal_loss\", \"kl_divergence\"]\n","\n","loss_fn = loss_functions[2]  # Change to desired loss function\n","wandb.init(project=\"bert-crossencoder-classification\", name=f\"bert-crossencoder-classification-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","train_crossencoder(loss_fn)\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
