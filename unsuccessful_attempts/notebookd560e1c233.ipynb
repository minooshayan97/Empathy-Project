{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q transformers datasets torch sentence_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!huggingface-cli login --token hf_"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, BertForSequenceClassification, Trainer\n","from datasets import load_dataset\n","\n","# Load the model and tokenizer\n","model_name = \"minoosh/empathy-crossencoder-cross_entropy\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name)\n","\n","# Load the dataset\n","dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n","\n","# Tokenize the test split\n","def preprocess_test_function(examples):\n","    encodings = tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n","    return encodings\n","\n","# Apply tokenization to the test dataset\n","tokenized_test = dataset['test'].map(preprocess_test_function, batched=True)\n","tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-11-01T01:27:13.295848Z","iopub.status.busy":"2024-11-01T01:27:13.294944Z","iopub.status.idle":"2024-11-01T01:27:14.755042Z","shell.execute_reply":"2024-11-01T01:27:14.754082Z","shell.execute_reply.started":"2024-11-01T01:27:13.295803Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import BertForSequenceClassification, AutoTokenizer, AutoConfig, TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load dataset\n","dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n","\n","# Initialize the tokenizer and model for cross-encoder setup\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Preprocess data for the cross-encoder model by concatenating text1 and text2 with [SEP]\n","def preprocess_function(examples):\n","    # Concatenate both texts with a [SEP] token in between\n","    encodings = tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n","    encodings['labels'] = examples['label']  # Add labels\n","    return encodings\n","\n","# Apply tokenization\n","tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n","\n","# Set format for PyTorch\n","tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# Define compute_metrics function for classification evaluation\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    preds = predictions.argmax(axis=1)\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average=\"weighted\")\n","    recall = recall_score(labels, preds, average=\"weighted\")\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1\n","    }\n","\n","# Custom Cross-Encoder model class for classification\n","class CrossEncoderModel(torch.nn.Module):\n","    def __init__(self, model_name, num_classes=4, loss_fn=\"cross_entropy\"):\n","        super(CrossEncoderModel, self).__init__()\n","        # Load model config\n","        self.config = AutoConfig.from_pretrained(model_name, num_labels=num_classes)\n","        self.model = BertForSequenceClassification.from_pretrained(model_name, config=self.config)\n","        self.loss_fn = loss_fn\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Output logits for classification\n","\n","        loss = None\n","        if labels is not None:\n","            if self.loss_fn == \"cross_entropy\":\n","                loss_fct = torch.nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n","                loss = loss_fct(logits, labels)\n","            elif self.loss_fn == \"focal_loss\":\n","                # Focal loss implementation for handling class imbalance\n","                alpha = 0.25\n","                gamma = 2.0\n","                ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, labels)\n","                pt = torch.exp(-ce_loss)  # Probability of the true class\n","                loss = (alpha * (1 - pt) ** gamma * ce_loss).mean()\n","            elif self.loss_fn == \"kl_divergence\":\n","                # KL Divergence for soft-label classification\n","                kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","                soft_labels = torch.nn.functional.one_hot(labels, num_classes=self.config.num_labels).float()\n","                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","                loss = kl_div(log_probs, soft_labels)\n","            else:\n","                raise ValueError(f\"Unsupported loss function: {self.loss_fn}\")\n","\n","        return {\"loss\": loss, \"logits\": logits}\n","\n","# Function to initialize and train the cross-encoder model\n","def test_crossencoder(model_name):\n","    \n","    model = CrossEncoderModel(model_name=model_name)\n","\n","    # Initialize the Trainer\n","    trainer = Trainer(model=model)\n","    \n","    # Make predictions on the test dataset\n","    predictions = trainer.predict(tokenized_test)\n","    \n","    # Get the predicted class indices\n","    predicted_classes = predictions.predictions.argmax(axis=1)\n","    \n","    # If you want to compare with actual labels\n","    actual_labels = tokenized_test['label']\n","    \n","    # Print out predictions and actual labels for verification\n","    for i in range(len(predicted_classes)):\n","        print(f\"Predicted: {predicted_classes[i]}, Actual: {actual_labels[i]}\")\n","\n","    # Save and push the model to the Hugging Face Hub\n","    #trainer.save_model(f\"./output/empathy-crossencoder-{loss_fn}\")\n","    #trainer.push_to_hub(f\"minoosh/empathy-crossencoder-{loss_fn}\")\n","    \n","    return predicted_classes, actual_labels"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-11-01T01:27:18.837828Z","iopub.status.busy":"2024-11-01T01:27:18.837290Z","iopub.status.idle":"2024-11-01T01:27:25.388861Z","shell.execute_reply":"2024-11-01T01:27:25.387952Z","shell.execute_reply.started":"2024-11-01T01:27:18.837777Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at minoosh/bert_empathy-crossencoder-focal_loss and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 3\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 1\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 0\n","Predicted: 2, Actual: 2\n","Predicted: 2, Actual: 1\n"]}],"source":["a,b = test_crossencoder(\"minoosh/bert_empathy-crossencoder-focal_loss\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-11-01T01:28:05.752454Z","iopub.status.busy":"2024-11-01T01:28:05.751712Z","iopub.status.idle":"2024-11-01T01:28:05.773335Z","shell.execute_reply":"2024-11-01T01:28:05.772390Z","shell.execute_reply.started":"2024-11-01T01:28:05.752408Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"data":{"text/plain":["{'accuracy': 0.4155844155844156,\n"," 'precision': 0.17383923266276208,\n"," 'recall': 0.4155844155844156,\n"," 'f1': 0.24513735112813453}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["def compute_metrics(predictions, labels):\n","    #predictions, labels = eval_pred\n","    preds = predictions\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average=\"weighted\")\n","    recall = recall_score(labels, preds, average=\"weighted\")\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1\n","    }\n","\n","compute_metrics(a,b)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-11-01T01:25:17.979034Z","iopub.status.busy":"2024-11-01T01:25:17.978635Z","iopub.status.idle":"2024-11-01T01:25:17.986149Z","shell.execute_reply":"2024-11-01T01:25:17.985338Z","shell.execute_reply.started":"2024-11-01T01:25:17.978997Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'eval_loss': 1.3570916652679443,\n"," 'eval_runtime': 5.0908,\n"," 'eval_samples_per_second': 60.501,\n"," 'eval_steps_per_second': 3.929}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["a"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-11-01T01:20:38.279243Z","iopub.status.busy":"2024-11-01T01:20:38.278854Z","iopub.status.idle":"2024-11-01T01:21:01.386230Z","shell.execute_reply":"2024-11-01T01:21:01.385131Z","shell.execute_reply.started":"2024-11-01T01:20:38.279197Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at minoosh/bert_empathy-crossencoder-focal_loss and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminooshayan97\u001b[0m (\u001b[33mminoosh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"840f6cd123384c8197c048b8d1e2a1d5","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112870988889275, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241101_012058-eih0xsfu</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/huggingface/runs/eih0xsfu' target=\"_blank\">tmp_trainer</a></strong> to <a href='https://wandb.ai/minoosh/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/huggingface' target=\"_blank\">https://wandb.ai/minoosh/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/huggingface/runs/eih0xsfu' target=\"_blank\">https://wandb.ai/minoosh/huggingface/runs/eih0xsfu</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 3\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 1\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 0\n","Predicted: 1, Actual: 2\n","Predicted: 1, Actual: 1\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","from datasets import load_dataset\n","\n","# Load the model and tokenizer\n","#minoosh/bert_empathy-crossencoder-focal_loss\n","#bert-base-uncased\n","#minoosh/empathy-crossencoder-cross_entropy\n","tokenizer = AutoTokenizer.from_pretrained(\"minoosh/bert_empathy-crossencoder-focal_loss\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"minoosh/bert_empathy-crossencoder-focal_loss\")\n","\n","# Load the dataset\n","dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n","\n","# Tokenize the test split\n","def preprocess_test_function(examples):\n","    encodings = tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n","    return encodings\n","\n","# Apply tokenization to the test dataset\n","tokenized_test = dataset['test'].map(preprocess_test_function, batched=True)\n","tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","\n","'''dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n","\n","# Preprocess data for the cross-encoder model by concatenating text1 and text2 with [SEP]\n","def preprocess_function(examples):\n","    # Concatenate both texts with a [SEP] token in between\n","    encodings = tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n","    #encodings['labels'] = examples['label']  # Add labels\n","    return encodings\n","\n","# Apply tokenization\n","tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","tokenized_val = dataset['validation'].map(preprocess_function, batched=True)'''\n","\n","\n","\n","\n","# Initialize the Trainer\n","trainer = Trainer(model=model)\n","\n","# Make predictions on the test dataset\n","predictions = trainer.predict(tokenized_test)\n","\n","# Get the predicted class indices\n","predicted_classes = predictions.predictions.argmax(axis=1)\n","\n","# If you want to compare with actual labels\n","actual_labels = tokenized_test['label']\n","\n","# Print out predictions and actual labels for verification\n","for i in range(len(predicted_classes)):\n","    print(f\"Predicted: {predicted_classes[i]}, Actual: {actual_labels[i]}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-01T01:21:45.652087Z","iopub.status.busy":"2024-11-01T01:21:45.651563Z","iopub.status.idle":"2024-11-01T01:21:50.615454Z","shell.execute_reply":"2024-11-01T01:21:50.614438Z","shell.execute_reply.started":"2024-11-01T01:21:45.652021Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'eval_runtime': 4.9493,\n"," 'eval_samples_per_second': 62.231,\n"," 'eval_steps_per_second': 4.041}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate(tokenized_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
