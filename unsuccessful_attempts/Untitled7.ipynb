{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ-GauKcHGRe"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGlt89sGHRUU"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token hf_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9bJEaQeHDpV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import wandb\n",
        "from transformers import BertConfig\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"bert-crossencoder-empathy\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize both sentences together, separated by [SEP]\n",
        "def preprocess_function(examples):\n",
        "    # Encode both sentences as a single input\n",
        "    return tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n",
        "tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n",
        "tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Remove unnecessary columns and set format for PyTorch\n",
        "columns_to_keep = ['input_ids', 'attention_mask', 'label']\n",
        "tokenized_train.set_format(type='torch', columns=columns_to_keep)\n",
        "tokenized_test.set_format(type='torch', columns=columns_to_keep)\n",
        "tokenized_val.set_format(type='torch', columns=columns_to_keep)\n",
        "\n",
        "# Define the compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.squeeze()\n",
        "    labels = labels.squeeze()\n",
        "\n",
        "    mse = mean_squared_error(labels, predictions)\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    pearson_corr, _ = pearsonr(predictions, labels)\n",
        "    spearman_corr, _ = spearmanr(predictions, labels)\n",
        "\n",
        "    return {\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"pearson_corr\": pearson_corr,\n",
        "        \"spearman_corr\": spearman_corr\n",
        "    }\n",
        "\n",
        "# Custom Cosine Similarity Loss\n",
        "class CosineSimilarityLoss(torch.nn.Module):\n",
        "    def forward(self, predictions, targets):\n",
        "        cos_sim = torch.nn.functional.cosine_similarity(predictions, targets, dim=0)\n",
        "        return 1 - cos_sim.mean()  # Minimize (1 - cosine similarity) for similarity maximization\n",
        "\n",
        "# CrossEncoder Model with SequenceClassification head\n",
        "def train_crossencoder(loss_fn=\"mse\"):\n",
        "    # Load pre-trained BERT model for sequence classification with config\n",
        "    config = BertConfig.from_pretrained(model_name, num_labels=1)  # Set num_labels=1 for regression\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "    # Define the loss function based on the selected loss\n",
        "    def custom_loss_fn(logits, labels):\n",
        "        logits = logits.squeeze()  # Ensure correct shape for loss calculation\n",
        "        if loss_fn == \"mse\":\n",
        "            loss_fct = torch.nn.MSELoss()  # Mean Squared Error Loss\n",
        "        elif loss_fn == \"mae\":\n",
        "            loss_fct = torch.nn.L1Loss()  # Mean Absolute Error Loss\n",
        "        elif loss_fn == \"cross_entropy\":\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()  # Cross-Entropy Loss for classification\n",
        "            labels = labels.long()  # Convert labels to long for CrossEntropy\n",
        "        elif loss_fn == \"cosine_sim\":\n",
        "            loss_fct = CosineSimilarityLoss()  # Custom Cosine Similarity Loss\n",
        "        return loss_fct(logits, labels)\n",
        "\n",
        "    # Wrap the custom loss function in a Trainer-compatible format\n",
        "    class CustomTrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False):\n",
        "            labels = inputs.pop(\"labels\")\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.get(\"logits\")\n",
        "            loss = custom_loss_fn(logits, labels)\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    # Define TrainingArguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./output/crossencoder-{loss_fn}\",\n",
        "        evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n",
        "        logging_dir='./logs',           # Directory for logs\n",
        "        logging_steps=10,               # Log every 10 steps\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        warmup_steps=100,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"wandb\",\n",
        "        save_strategy=\"epoch\",          # Save checkpoints at the end of each epoch\n",
        "        save_total_limit=2,             # Keep only the 2 most recent checkpoints\n",
        "        push_to_hub=True                # Automatically push to Hugging Face Hub\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer with custom loss function\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    trainer.evaluate(tokenized_test)\n",
        "\n",
        "    # Save the model locally and to Hugging Face Hub\n",
        "    trainer.save_model(f\"./output/crossencoder-{loss_fn}\")\n",
        "    trainer.push_to_hub(f\"minoosh/crossencoder-{loss_fn}\")\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KziROPdHhHx"
      },
      "outputs": [],
      "source": [
        "# Train cross-encoders with different loss functions\n",
        "loss_functions = [\"mse\", \"mae\", \"cross_entropy\", \"cosine_sim\"]\n",
        "\n",
        "loss_fn = loss_functions[0]\n",
        "# Initialize wandb with a unique run name for each loss function\n",
        "wandb.init(project=\"bert-crossencoder-empathy\", name=f\"cross_encoder_{loss_fn}_run\", config={\"epochs\": 3, \"batch_size\": 16, \"learning_rate\": 2e-5})\n",
        "train_crossencoder(loss_fn)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
