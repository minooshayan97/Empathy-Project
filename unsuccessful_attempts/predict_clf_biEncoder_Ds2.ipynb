{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tgj3xkNh7RF",
        "outputId": "0d53c73b-8a73-4c16-9be1-5633d980f6f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpS1JGLpiiAE",
        "outputId": "81d8fc7a-2075-4e6b-a101-c5e4132be257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0ko5hr-koyu"
      },
      "outputs": [],
      "source": [
        "# Define a custom BiEncoder model with options for different loss functions\n",
        "class BiEncoderModel(torch.nn.Module):\n",
        "    def __init__(self, base_model, config=None, num_classes=4, loss_fn=\"cross_entropy\"):\n",
        "        super(BiEncoderModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.config = config  # Add this line to set the config attribute\n",
        "        self.classifier = torch.nn.Linear(base_model.config.hidden_size * 2, num_classes)  # Updated for 4 classes\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def forward(self, input_ids_text1, attention_mask_text1, input_ids_text2, attention_mask_text2, labels=None):\n",
        "        # Encode text1 and text2 separately\n",
        "        outputs_text1 = self.base_model(input_ids_text1, attention_mask=attention_mask_text1)\n",
        "        outputs_text2 = self.base_model(input_ids_text2, attention_mask=attention_mask_text2)\n",
        "        # Extract [CLS] token embeddings (first token)\n",
        "        cls_embedding_text1 = outputs_text1.last_hidden_state[:, 0, :]\n",
        "        cls_embedding_text2 = outputs_text2.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Concatenate embeddings and apply classifier\n",
        "        concatenated_embeddings = torch.cat([cls_embedding_text1, cls_embedding_text2], dim=1)\n",
        "        logits = self.classifier(concatenated_embeddings)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.loss_fn == \"cross_entropy\":\n",
        "                loss_fct = torch.nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "                loss = loss_fct(logits, labels)\n",
        "            elif self.loss_fn == \"focal_loss\":\n",
        "                # Focal loss implementation\n",
        "                alpha = 0.25\n",
        "                gamma = 2.0\n",
        "                ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, labels)\n",
        "                pt = torch.exp(-ce_loss)  # Probability of the true class\n",
        "                loss = (alpha * (1 - pt) ** gamma * ce_loss).mean()\n",
        "            elif self.loss_fn == \"kl_divergence\":\n",
        "                # KL Divergence for soft-label classification\n",
        "                kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
        "                soft_labels = torch.nn.functional.one_hot(labels, num_classes=self.classifier.out_features).float()\n",
        "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "                loss = kl_div(log_probs, soft_labels)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported loss function: {self.loss_fn}\")\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou01osGGlcWI"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n",
        "    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n",
        "    return {\n",
        "        'input_ids_text1': text1_encodings['input_ids'],\n",
        "        'attention_mask_text1': text1_encodings['attention_mask'],\n",
        "        'input_ids_text2': text2_encodings['input_ids'],\n",
        "        'attention_mask_text2': text2_encodings['attention_mask'],\n",
        "        'labels': examples['label']\n",
        "    }\n",
        "\n",
        "columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ6YZkd2hm0a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "\n",
        "def predict_test_set(trainer, test_dataset):\n",
        "    # Get predictions\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    pred_logits = predictions.predictions\n",
        "    pred_labels = np.argmax(pred_logits, axis=1)  # Get the predicted class labels\n",
        "    return pred_labels, predictions.label_ids  # Return predicted and actual labels\n",
        "\n",
        "\n",
        "def compute_metrics2(predictions, labels):\n",
        "    mse = mean_squared_error(labels, predictions)\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    pearson_corr, _ = pearsonr(predictions, labels)\n",
        "    spearman_corr, _ = spearmanr(predictions, labels)\n",
        "    cosine_sim = torch.nn.functional.cosine_similarity(torch.tensor(predictions), torch.tensor(labels), dim=0).mean().item()\n",
        "\n",
        "    return {\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"pearson_corr\": pearson_corr,\n",
        "        \"spearman_corr\": spearman_corr,\n",
        "        \"cosine_sim\": cosine_sim  # Optional metric for similarity tasks\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTrZ0FrqbJuT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from huggingface_hub import HfApi\n",
        "from transformers import AutoModel, AutoConfig, AutoTokenizer, BertConfig\n",
        "\n",
        "\n",
        "def load_from_hub(repo_id, num_classes=4):\n",
        "    \"\"\"\n",
        "    Load BiEncoder model from Hugging Face Hub\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Loading model from {repo_id}...\")\n",
        "\n",
        "        # 1. Initialize base model with BERT\n",
        "        base_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # 2. Create BiEncoder model\n",
        "        model = BiEncoderModel(\n",
        "            base_model=base_model,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        # 3. Load state dict\n",
        "        state_dict = torch.hub.load_state_dict_from_url(\n",
        "            f\"https://huggingface.co/{repo_id}/resolve/main/pytorch_model.bin\",\n",
        "            map_location=\"cpu\"\n",
        "        )\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        # 4. Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "        # 5. Create trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            data_collator=BiEncoderCollator(),\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        return trainer, model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from hub: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5O-4xc-qwbV"
      },
      "source": [
        "# \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0_Em1jDcjYc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load test dataset\n",
        "dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n",
        "\n",
        "\n",
        "# Tokenize the test set\n",
        "tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Set the format for the test dataset for PyTorch\n",
        "tokenized_test.set_format(type='torch', columns=columns_to_keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi9enqLmbRIL"
      },
      "outputs": [],
      "source": [
        "model = \"repo_id\"\n",
        "loaded_trainer, loaded_model, loaded_tokenizer = load_from_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJeMCssxlKAP"
      },
      "outputs": [],
      "source": [
        "pred_labels, true_labels = predict_test_set(loaded_trainer, tokenized_test)\n",
        "\n",
        "compute_metrics2(preds, dataset['test']['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZGv0Dp_rGdc"
      },
      "source": [
        "# \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhcqW-k2b7LG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU3Z-7SDb83m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yIRB1O2lDev"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeDWeF4hlDio"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V00xn-scCbN"
      },
      "source": [
        "# \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP9xDwVXlFEP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0aXAgrGlFJH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sadfT-sElHHn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-HhPGtLlFOP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
