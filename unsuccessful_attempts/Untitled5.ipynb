{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOcJphj8ntpA",
        "outputId": "b8ef0f5e-40b2-40ea-9611-4e527e0f93fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution - (c:\\users\\hai_l\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\hai_l\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\users\\hai_l\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\hai_l\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\users\\hai_l\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\hai_l\\appdata\\roaming\\python\\python39\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets wandb sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qwPHCOdKabq",
        "outputId": "7da59d16-25ec-4be0-ce5c-43150a667f67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'huggingface-cli' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token hf_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7p2OsK5FidO",
        "outputId": "43745328-2bff-4669-d89b-c2440ec4d741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hai_l\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Initialize CrossEncoder model (BERT-based) with regression output\n",
        "model = CrossEncoder('bert-base-uncased', num_labels=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "yVSz74nrDHd0",
        "outputId": "ef0d1d65-40d5-48bb-90f1-5dbf12f677d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\hai_l\\OneDrive\\Desktop\\Empathy Recognition\\wandb\\run-20241023_145151-23rws9gu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/minoosh/TESTbert-crossencoder-empathy/runs/23rws9gu' target=\"_blank\">wandering-sun-7</a></strong> to <a href='https://wandb.ai/minoosh/TESTbert-crossencoder-empathy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/minoosh/TESTbert-crossencoder-empathy' target=\"_blank\">https://wandb.ai/minoosh/TESTbert-crossencoder-empathy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/minoosh/TESTbert-crossencoder-empathy/runs/23rws9gu' target=\"_blank\">https://wandb.ai/minoosh/TESTbert-crossencoder-empathy/runs/23rws9gu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/minoosh/TESTbert-crossencoder-empathy/runs/23rws9gu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x18c51cfb940>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"TESTbert-crossencoder-empathy\",\n",
        "    config={\"epochs\": 3, \"batch_size\": 16, \"learning_rate\": 2e-5}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL_jl36aC1Ct",
        "outputId": "ba93493f-f82f-4b08-9281-da47087e4fc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Convert to PyTorch tensors\\ntokenized_train.set_format(\"torch\")\\ntokenized_test.set_format(\"torch\")\\ntokenized_val.set_format(\"torch\")'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n",
        "\n",
        "train_examples = [InputExample(texts=[row['text1'], row['text2']], label=row['label']) for row in dataset['train']]\n",
        "val_examples = [InputExample(texts=[row['text1'], row['text2']], label=row['label']) for row in dataset['validation']]\n",
        "\n",
        "# Tokenize the dataset for CrossEncoder\n",
        "def preprocess_function(examples):\n",
        "    return model.tokenizer(\n",
        "        examples['text1'],\n",
        "        examples['text2'],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# Apply tokenization to each split in the dataset\n",
        "tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n",
        "tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n",
        "tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Remove unneeded columns and set format for PyTorch\n",
        "tokenized_train = tokenized_train.remove_columns([\"text1\", \"text2\"])\n",
        "tokenized_test = tokenized_test.remove_columns([\"text1\", \"text2\"])\n",
        "tokenized_val = tokenized_val.remove_columns([\"text1\", \"text2\"])\n",
        "\n",
        "'''# Convert to PyTorch tensors\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")\n",
        "tokenized_val.set_format(\"torch\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "SZlbm7goss-M",
        "outputId": "a9b4138c-5f5a-4252-c149-4fd032ba1efb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [123/123 04:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Pearson Corr</th>\n",
              "      <th>Spearman Corr</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.103800</td>\n",
              "      <td>0.074989</td>\n",
              "      <td>0.074989</td>\n",
              "      <td>0.229586</td>\n",
              "      <td>0.069260</td>\n",
              "      <td>0.053132</td>\n",
              "      <td>-0.033615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.096200</td>\n",
              "      <td>0.076002</td>\n",
              "      <td>0.076002</td>\n",
              "      <td>0.228981</td>\n",
              "      <td>0.229442</td>\n",
              "      <td>0.103004</td>\n",
              "      <td>-0.047581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.104800</td>\n",
              "      <td>0.096649</td>\n",
              "      <td>0.096649</td>\n",
              "      <td>0.249318</td>\n",
              "      <td>0.260964</td>\n",
              "      <td>0.208432</td>\n",
              "      <td>-0.332173</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=123, training_loss=0.10035543180093533, metrics={'train_runtime': 251.1591, 'train_samples_per_second': 7.68, 'train_steps_per_second': 0.49, 'total_flos': 507536668781568.0, 'train_loss': 0.10035543180093533, 'epoch': 3.0})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from MyCEmetrics import compute_metrics\n",
        "\n",
        "# Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output/empathy-crossencoder\",  # Save model here\n",
        "    eval_strategy=\"steps\",                 # Evaluate model every few steps\n",
        "    eval_steps=40,                              # Evaluation frequency\n",
        "    logging_dir='./logs',                        # Directory for logs\n",
        "    logging_steps=10,                            # Log every 10 steps\n",
        "    per_device_train_batch_size=wandb.config['batch_size'],\n",
        "    per_device_eval_batch_size=wandb.config['batch_size'],\n",
        "    num_train_epochs=wandb.config['epochs'],\n",
        "    warmup_steps=100,                            # Warmup steps\n",
        "    learning_rate=wandb.config['learning_rate'],  # Learning rate\n",
        "    weight_decay=0.01,                           # Prevent overfitting\n",
        "    report_to=\"wandb\",                           # Log to wandb\n",
        "    save_steps=40,                              # Save every 40 steps\n",
        "    save_total_limit=2,                          # Save only the 2 latest checkpoints\n",
        "    load_best_model_at_end=True,                 # Load the best model at the end\n",
        "    metric_for_best_model=\"mse\",                 # Use MSE as the metric\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Trainer for training the CrossEncoder model\n",
        "trainer = Trainer(\n",
        "    model=model.model,                    # CrossEncoder model (inner model)\n",
        "    args=training_args,                   # Training arguments\n",
        "    train_dataset=tokenized_train,         # Training dataset\n",
        "    eval_dataset=tokenized_val,            # Validation dataset\n",
        "    tokenizer=model.tokenizer,            # Tokenizer from CrossEncoder\n",
        "    compute_metrics=compute_metrics,       # Function to compute metrics\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHdNBpXDMRHR"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Evaluate on the test set\n",
        "trainer.evaluate(tokenized_test)\n",
        "\n",
        "# Save the model to Hugging Face Hub\n",
        "model.push_to_hub(\"minoosh/crossencoder-empathy-model1\")\n",
        "model.tokenizer.push_to_hub(\"minoosh/crossencoder-empathy-model1\")\n",
        "\n",
        "# Get predictions on the test set\n",
        "predictions_output = trainer.predict(tokenized_test)\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()\n",
        "\n",
        "with open('predictions.pkl', 'wb') as f:\n",
        "    pickle.dump(predictions_output, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LutFAe-7SaE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
