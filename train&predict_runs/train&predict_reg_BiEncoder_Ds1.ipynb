{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-06T01:14:19.741453Z","iopub.status.busy":"2024-11-06T01:14:19.740660Z","iopub.status.idle":"2024-11-06T01:14:33.070524Z","shell.execute_reply":"2024-11-06T01:14:33.069290Z","shell.execute_reply.started":"2024-11-06T01:14:19.741417Z"},"executionInfo":{"elapsed":13665,"status":"ok","timestamp":1729811889433,"user":{"displayName":"Minoo Shayan","userId":"09011516747950461668"},"user_tz":420},"id":"aP-2A1gCkqsq","outputId":"0d6244aa-e3e2-4355-ca79-a7932f87f75f","trusted":true},"outputs":[],"source":["!pip install -q transformers datasets wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-06T01:14:33.072871Z","iopub.status.busy":"2024-11-06T01:14:33.072527Z","iopub.status.idle":"2024-11-06T01:14:34.669293Z","shell.execute_reply":"2024-11-06T01:14:34.668204Z","shell.execute_reply.started":"2024-11-06T01:14:33.072834Z"},"executionInfo":{"elapsed":3499,"status":"ok","timestamp":1729811898145,"user":{"displayName":"Minoo Shayan","userId":"09011516747950461668"},"user_tz":420},"id":"0VA7Mpf1ktNB","outputId":"ce56f53a-8e22-4ba5-b482-cc1751bbcecf","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token hf_"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:14:44.401673Z","iopub.status.busy":"2024-11-06T01:14:44.401157Z","iopub.status.idle":"2024-11-06T01:15:20.150122Z","shell.execute_reply":"2024-11-06T01:15:20.149233Z","shell.execute_reply.started":"2024-11-06T01:14:44.401633Z"},"id":"tsK_q1ackMS0","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c0b35617334430c8444340c812ab894","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113643588888382, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_011507-clishih0</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/clishih0' target=\"_blank\">curious-butterfly-45</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/clishih0' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/clishih0</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4959fafcdf77409f9dcaa0d0169b433d","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/589 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b434a8c5cab6479d80850e201f70fcc8","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03afc5180cad4e64ad4b855dddb716e1","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/118k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a14ef69b1ee5458888bc55d343488db3","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/118k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a88b1964470e4f1e8726a2a23526af59","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/643 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff144d2cdef14f25b66ee331de0c9df0","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00e7356d6c4a438ba5389d6707b09c65","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/81 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4620fcb866944036805d4ec119a483d0","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f62474d75c84bf6ab0c9d0081d115e6","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4770f1fecd14ab3b7ac49552f891687","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3f9b132d1234fe8b3af91217223050e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faf635d3d20d426998feda29b74297f5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8a1a5b16f12483287e08ab1b1a1413c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/643 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbe0fc7400194107abf903e06abd6d96","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/81 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n","from transformers import BertConfig, BertModel\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.stats import pearsonr, spearmanr\n","import wandb\n","import numpy as np\n","\n","# Initialize wandb\n","wandb.init(\n","    project=\"bert-biencoder-regression\"\n",")\n","\n","\n","# Load dataset\n","dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n","\n","\n","# Initialize bi-encoder model (e.g., BERT as a sentence encoder)\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","base_model = AutoModel.from_pretrained(model_name)\n","\n","\n","# Tokenize both text1 and text2 independently\n","def preprocess_function(examples):\n","    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n","    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n","    return {\n","        'input_ids_text1': text1_encodings['input_ids'],\n","        'attention_mask_text1': text1_encodings['attention_mask'],\n","        'input_ids_text2': text2_encodings['input_ids'],\n","        'attention_mask_text2': text2_encodings['attention_mask'],\n","        'labels': examples['label']\n","    }\n","\n","\n","\n","# Apply tokenization\n","tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n","#tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n","\n","# Remove unnecessary columns and set format for PyTorch\n","columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']\n","tokenized_train.set_format(type='torch', columns=columns_to_keep)\n","#tokenized_test.set_format(type='torch', columns=columns_to_keep)\n","tokenized_val.set_format(type='torch', columns=columns_to_keep)\n","\n","\n","\n","# Define a custom collator to handle text1 and text2 encoding\n","class BiEncoderCollator:\n","    def __call__(self, features):\n","        batch = {\n","            'input_ids_text1': torch.stack([f['input_ids_text1'] for f in features]),\n","            'attention_mask_text1': torch.stack([f['attention_mask_text1'] for f in features]),\n","            'input_ids_text2': torch.stack([f['input_ids_text2'] for f in features]),\n","            'attention_mask_text2': torch.stack([f['attention_mask_text2'] for f in features]),\n","            'labels': torch.tensor([f['labels'] for f in features], dtype=torch.float)\n","        }\n","        return batch\n","\n","\n","collator = BiEncoderCollator()\n","\n","\n","# Define the compute_metrics function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.squeeze()\n","    labels = labels.squeeze()\n","\n","    mse = mean_squared_error(labels, predictions)\n","    mae = mean_absolute_error(labels, predictions)\n","    pearson_corr, _ = pearsonr(predictions, labels)\n","    spearman_corr, _ = spearmanr(predictions, labels)\n","    cosine_sim = torch.nn.functional.cosine_similarity(torch.tensor(predictions), torch.tensor(labels), dim=0).mean().item()\n","\n","    return {\n","        \"mse\": mse,\n","        \"mae\": mae,\n","        \"pearson_corr\": pearson_corr,\n","        \"spearman_corr\": spearman_corr,\n","        \"cosine_sim\": cosine_sim  # Optional metric for similarity tasks\n","    }\n","\n","\n","# Define a custom BiEncoder model\n","class BiEncoderModel(torch.nn.Module):\n","    def __init__(self, base_model, config=None, loss_fn=\"mse\"):\n","        super(BiEncoderModel, self).__init__()\n","        self.base_model = base_model\n","        self.cos = torch.nn.CosineSimilarity(dim=1)\n","        self.loss_fn = loss_fn\n","        self.config = config\n","\n","\n","    def forward(self, input_ids_text1, attention_mask_text1, input_ids_text2, attention_mask_text2, labels=None):\n","        # Encode text1 and text2 separately\n","        outputs_text1 = self.base_model(input_ids_text1, attention_mask=attention_mask_text1)\n","        outputs_text2 = self.base_model(input_ids_text2, attention_mask=attention_mask_text2)\n","\n","\n","        # Extract [CLS] token embeddings (first token)\n","        cls_embedding_text1 = outputs_text1.last_hidden_state[:, 0, :]\n","        cls_embedding_text2 = outputs_text2.last_hidden_state[:, 0, :]\n","\n","\n","        # Calculate cosine similarity between the two embeddings\n","        cos_sim = self.cos(cls_embedding_text1, cls_embedding_text2)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.loss_fn == \"mse\":\n","                loss_fct = torch.nn.MSELoss()  # Mean Squared Error Loss\n","            elif self.loss_fn == \"mae\":\n","                loss_fct = torch.nn.L1Loss()  # Mean Absolute Error Loss\n","            elif self.loss_fn == \"contrastive\":\n","                loss_fct = self.contrastive_loss\n","            elif self.loss_fn == \"cosine_embedding\":\n","                loss_fct = torch.nn.CosineEmbeddingLoss()  # Cosine Embedding Loss\n","\n","\n","            if self.loss_fn == \"cosine_embedding\":\n","                labels_cosine = 2 * (labels > 0.5).float() - 1  # Convert labels to binary for cosine embedding loss\n","                loss = loss_fct(cls_embedding_text1, cls_embedding_text2, labels_cosine)\n","            else:\n","                loss = loss_fct(cos_sim, labels)\n","\n","        return {\"loss\": loss, \"logits\": cos_sim}\n","\n","\n","    def contrastive_loss(self, cos_sim, labels, margin=0.5):\n","        loss = torch.mean((1 - labels) * torch.pow(cos_sim, 2) + labels * torch.pow(torch.clamp(margin - cos_sim, min=0.0), 2))\n","        return loss\n","\n","\n","\n","# Initialize the Bi-Encoder model with a specific loss function\n","def train_biencoder(loss_fn):\n","    # Load pre-trained BERT configuration and model\n","    config = BertConfig.from_pretrained(model_name)\n","    bert_model = BertModel.from_pretrained(model_name)\n","\n","    # Initialize your custom BiEncoderModel with the BERT model and config\n","    bi_encoder_model = BiEncoderModel(base_model=bert_model, config=config, loss_fn=loss_fn)\n","    #bi_encoder_model = BiEncoderModel(base_model, loss_fn)\n","\n","    # Define TrainingArguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"./output/bert-reg-biencoder-{loss_fn}\",\n","        evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n","        logging_dir='./logs',           # Directory for logs\n","        logging_steps=10,               # Log every 10 steps\n","        per_device_train_batch_size=wandb.config['batch_size'],\n","        per_device_eval_batch_size=wandb.config['batch_size'],\n","        num_train_epochs=wandb.config['epochs'],\n","        warmup_steps=100,\n","        learning_rate=wandb.config['learning_rate'],\n","        weight_decay=0.01,\n","        report_to=\"wandb\",\n","        save_strategy=\"epoch\",          # Save checkpoints at the end of each epoch\n","        load_best_model_at_end=True,\n","        push_to_hub=True,\n","        save_total_limit=2              # Keep only the 2 most recent checkpoints\n","\n","    )\n","\n","\n","    # Define the Trainer\n","    trainer = Trainer(\n","        model=bi_encoder_model,             # Custom BiEncoder model\n","        args=training_args,                 # Training arguments\n","        train_dataset=tokenized_train,      # Training dataset\n","        eval_dataset=tokenized_val,         # Validation dataset\n","        data_collator=collator,             # Custom collator for handling bi-encoder inputs\n","        compute_metrics=compute_metrics     # Function to compute metrics\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","\n","    # Evaluate the model on the test set\n","    #trainer.evaluate(tokenized_test)\n","\n","    # Save the model to Hugging Face Hub\n","    trainer.save_model(f\"./output/bert-reg-biencoder-{loss_fn}\")\n","    trainer.push_to_hub(f\"minoosh/bert-reg-biencoder-{loss_fn}\")\n","\n","\n","\n","    # Finish wandb run\n","    wandb.finish()\n","\n","    return trainer"]},{"cell_type":"markdown","metadata":{},"source":["# 0"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-11-06T01:01:36.420049Z","iopub.status.busy":"2024-11-06T01:01:36.419735Z","iopub.status.idle":"2024-11-06T01:09:40.095968Z","shell.execute_reply":"2024-11-06T01:09:40.095270Z","shell.execute_reply.started":"2024-11-06T01:01:36.420017Z"},"id":"hKzcAt8eNLsa","outputId":"a1fadc49-f90c-44c5-ef49-1c280e5c616f","trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:misa8mvy) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42adbd02c17f47359fbc959dbe95ed37","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">playful-lion-42</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/misa8mvy' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/misa8mvy</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_010122-misa8mvy/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:misa8mvy). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_010136-ltyfhobk</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/ltyfhobk' target=\"_blank\">bert-biencoder-regression-mse</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/ltyfhobk' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/ltyfhobk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [147/147 07:37, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","      <th>Mae</th>\n","      <th>Pearson Corr</th>\n","      <th>Spearman Corr</th>\n","      <th>Cosine Sim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.121900</td>\n","      <td>0.112369</td>\n","      <td>0.111656</td>\n","      <td>0.255983</td>\n","      <td>0.140632</td>\n","      <td>0.099309</td>\n","      <td>0.905528</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.101700</td>\n","      <td>0.083762</td>\n","      <td>0.083299</td>\n","      <td>0.224776</td>\n","      <td>0.131244</td>\n","      <td>0.123856</td>\n","      <td>0.904485</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.087200</td>\n","      <td>0.077782</td>\n","      <td>0.077472</td>\n","      <td>0.220506</td>\n","      <td>0.251986</td>\n","      <td>0.137354</td>\n","      <td>0.909682</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.069400</td>\n","      <td>0.085977</td>\n","      <td>0.085593</td>\n","      <td>0.232805</td>\n","      <td>0.192284</td>\n","      <td>0.145615</td>\n","      <td>0.903664</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.053300</td>\n","      <td>0.095826</td>\n","      <td>0.095081</td>\n","      <td>0.241779</td>\n","      <td>0.308914</td>\n","      <td>0.225189</td>\n","      <td>0.913172</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.047800</td>\n","      <td>0.078237</td>\n","      <td>0.077783</td>\n","      <td>0.221579</td>\n","      <td>0.291349</td>\n","      <td>0.232497</td>\n","      <td>0.909628</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.038500</td>\n","      <td>0.081660</td>\n","      <td>0.081212</td>\n","      <td>0.227752</td>\n","      <td>0.283509</td>\n","      <td>0.233073</td>\n","      <td>0.909724</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba7a75119a814b70ac2c2b110433db8c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>▂▂▅▁█▅▅</td></tr><tr><td>eval/loss</td><td>█▂▁▃▅▁▂</td></tr><tr><td>eval/mae</td><td>█▂▁▃▅▁▂</td></tr><tr><td>eval/mse</td><td>█▂▁▃▅▁▂</td></tr><tr><td>eval/pearson_corr</td><td>▁▁▆▃█▇▇</td></tr><tr><td>eval/runtime</td><td>▁▇▆▅▅▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▂▃▄▄▄▁</td></tr><tr><td>eval/spearman_corr</td><td>▁▂▃▃███</td></tr><tr><td>eval/steps_per_second</td><td>█▂▃▄▄▄▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▄▄▇▅▅█▆▄▃▃▁▃▃</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▇▅▅▄▄▃▃▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>0.90972</td></tr><tr><td>eval/loss</td><td>0.08166</td></tr><tr><td>eval/mae</td><td>0.22775</td></tr><tr><td>eval/mse</td><td>0.08121</td></tr><tr><td>eval/pearson_corr</td><td>0.28351</td></tr><tr><td>eval/runtime</td><td>2.512</td></tr><tr><td>eval/samples_per_second</td><td>32.246</td></tr><tr><td>eval/spearman_corr</td><td>0.23307</td></tr><tr><td>eval/steps_per_second</td><td>1.194</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/grad_norm</td><td>0.77398</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0385</td></tr><tr><td>train_loss</td><td>0.07421</td></tr><tr><td>train_runtime</td><td>461.8839</td></tr><tr><td>train_samples_per_second</td><td>9.745</td></tr><tr><td>train_steps_per_second</td><td>0.318</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-biencoder-regression-mse</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/ltyfhobk' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/ltyfhobk</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_010136-ltyfhobk/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Train bi-encoder with different loss functions\n","loss_functions = [\"mse\", \"mae\", \"contrastive\", \"cosine_embedding\"]\n","loss_fn = loss_functions[0]\n","wandb.init(project=\"bert-biencoder-regression\", name=f\"bert-biencoder-regression-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","tr = train_biencoder(loss_fn)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:09:40.120121Z","iopub.status.busy":"2024-11-06T01:09:40.119823Z","iopub.status.idle":"2024-11-06T01:09:43.097514Z","shell.execute_reply":"2024-11-06T01:09:43.096519Z","shell.execute_reply.started":"2024-11-06T01:09:40.120078Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to temp_save_bert-reg-biencoder-mse...\n","Saving tokenizer...\n","Pushing to hub at minoosh/bert-reg-biencoder-mse...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:9532: UserWarning: Warnings while validating metadata in README.md:\n","- The pipeline tag \"text-similarity\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, text2text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, any-to-any, other\n","  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"]},{"name":"stdout","output_type":"stream","text":["Successfully pushed model to minoosh/bert-reg-biencoder-mse\n"]}],"source":["tr.tokenizer = tokenizer\n","repo_id = f\"minoosh/bert-reg-biencoder-{loss_fn}\" \n","save_and_push_to_hub(tr, repo_id)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:09:43.099090Z","iopub.status.busy":"2024-11-06T01:09:43.098773Z","iopub.status.idle":"2024-11-06T01:09:44.655564Z","shell.execute_reply":"2024-11-06T01:09:44.654640Z","shell.execute_reply.started":"2024-11-06T01:09:43.099056Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3daac7a39684e7b9e3ece4515e34284","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load dataset\n","dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n","\n","# Tokenize both text1 and text2 independently\n","def preprocess_function(examples):\n","    #tokenizer = loaded_tokenizer\n","    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n","    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n","    return {\n","        'input_ids_text1': text1_encodings['input_ids'],\n","        'attention_mask_text1': text1_encodings['attention_mask'],\n","        'input_ids_text2': text2_encodings['input_ids'],\n","        'attention_mask_text2': text2_encodings['attention_mask'],\n","        'labels': examples['label']\n","    }\n","\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","\n","# Remove unnecessary columns and set format for PyTorch\n","columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']\n","tokenized_test.set_format(type='torch', columns=columns_to_keep)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:09:44.657381Z","iopub.status.busy":"2024-11-06T01:09:44.656668Z","iopub.status.idle":"2024-11-06T01:09:48.512993Z","shell.execute_reply":"2024-11-06T01:09:48.512060Z","shell.execute_reply.started":"2024-11-06T01:09:44.657320Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminooshayan97\u001b[0m (\u001b[33mminoosh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_010944-x7z2m3p8</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/uncategorized/runs/x7z2m3p8' target=\"_blank\">clear-water-3</a></strong> to <a href='https://wandb.ai/minoosh/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/uncategorized' target=\"_blank\">https://wandb.ai/minoosh/uncategorized</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/uncategorized/runs/x7z2m3p8' target=\"_blank\">https://wandb.ai/minoosh/uncategorized/runs/x7z2m3p8</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["PredictionOutput(predictions=array([0.5411869 , 0.72022855, 0.55109036, 0.7318852 , 0.6615393 ,\n","       0.6674884 , 0.6390757 , 0.54861987, 0.6876057 , 0.7989178 ,\n","       0.65871495, 0.7100351 , 0.7401358 , 0.701087  , 0.632648  ,\n","       0.6298427 , 0.6468965 , 0.51127005, 0.5996416 , 0.58648777,\n","       0.6594692 , 0.71406525, 0.70586884, 0.62428755, 0.7296514 ,\n","       0.7332828 , 0.6785795 , 0.6275251 , 0.68053985, 0.6697133 ,\n","       0.78167963, 0.667819  , 0.48748174, 0.6629039 , 0.56138086,\n","       0.71961564, 0.74248636, 0.6166283 , 0.6198367 , 0.5173968 ,\n","       0.51861715, 0.6012773 , 0.39831746, 0.70240736, 0.7234752 ,\n","       0.75781965, 0.60142493, 0.74426496, 0.60273045, 0.72714597,\n","       0.7692679 , 0.62940055, 0.7208171 , 0.7243372 , 0.69598484,\n","       0.74242675, 0.6271779 , 0.5507619 , 0.73643994, 0.71929026,\n","       0.7324075 , 0.41290402, 0.69370973, 0.77956635, 0.72937536,\n","       0.49146456, 0.74269587, 0.3367164 , 0.79455215, 0.6604707 ,\n","       0.58996105, 0.7397574 , 0.65962553, 0.63284415, 0.64485836,\n","       0.32750142, 0.56462485, 0.7494947 , 0.42716256, 0.586398  ],\n","      dtype=float32), label_ids=array([0.75, 0.5 , 0.5 , 1.  , 0.  , 0.25, 0.5 , 0.5 , 1.  , 0.5 , 0.75,\n","       0.5 , 0.75, 1.  , 0.  , 0.75, 0.5 , 0.5 , 0.25, 0.25, 0.75, 0.25,\n","       0.25, 1.  , 0.5 , 0.5 , 1.  , 0.5 , 0.25, 0.25, 1.  , 1.  , 1.  ,\n","       0.75, 0.75, 0.75, 0.75, 0.5 , 0.75, 0.5 , 0.  , 1.  , 0.  , 0.75,\n","       0.75, 1.  , 0.75, 0.  , 0.75, 1.  , 0.75, 0.75, 0.25, 0.5 , 0.5 ,\n","       0.25, 0.75, 0.75, 0.25, 1.  , 0.25, 0.5 , 0.75, 1.  , 0.5 , 1.  ,\n","       1.  , 0.  , 1.  , 0.75, 0.5 , 1.  , 0.  , 0.75, 0.  , 0.25, 0.75,\n","       0.75, 0.25, 0.5 ], dtype=float32), metrics={'test_loss': 0.09460709989070892, 'test_mse': 0.09460711479187012, 'test_mae': 0.24940118193626404, 'test_pearson_corr': 0.27960533757027406, 'test_spearman_corr': 0.22053166924329728, 'test_cosine_sim': 0.8917081356048584, 'test_runtime': 2.3781, 'test_samples_per_second': 33.64, 'test_steps_per_second': 1.261})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init()\n","tr.predict(tokenized_test)"]},{"cell_type":"markdown","metadata":{},"source":["# 1"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:15:20.151914Z","iopub.status.busy":"2024-11-06T01:15:20.151618Z","iopub.status.idle":"2024-11-06T01:23:41.676479Z","shell.execute_reply":"2024-11-06T01:23:41.675780Z","shell.execute_reply.started":"2024-11-06T01:15:20.151882Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:clishih0) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88e4abd0291242c79afb6e940e3695bd","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">curious-butterfly-45</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/clishih0' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/clishih0</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_011507-clishih0/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:clishih0). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_011520-irodxcvg</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/irodxcvg' target=\"_blank\">bert-biencoder-regression-mae</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/irodxcvg' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/irodxcvg</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [147/147 07:51, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","      <th>Mae</th>\n","      <th>Pearson Corr</th>\n","      <th>Spearman Corr</th>\n","      <th>Cosine Sim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.284600</td>\n","      <td>0.261682</td>\n","      <td>0.115271</td>\n","      <td>0.260970</td>\n","      <td>0.132692</td>\n","      <td>0.093613</td>\n","      <td>0.905303</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.272800</td>\n","      <td>0.230977</td>\n","      <td>0.088586</td>\n","      <td>0.230360</td>\n","      <td>0.018760</td>\n","      <td>0.031608</td>\n","      <td>0.899428</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.251100</td>\n","      <td>0.228228</td>\n","      <td>0.084699</td>\n","      <td>0.227604</td>\n","      <td>0.171599</td>\n","      <td>0.111065</td>\n","      <td>0.905807</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.225300</td>\n","      <td>0.233306</td>\n","      <td>0.086442</td>\n","      <td>0.232866</td>\n","      <td>0.190620</td>\n","      <td>0.119114</td>\n","      <td>0.904066</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.199300</td>\n","      <td>0.232854</td>\n","      <td>0.082225</td>\n","      <td>0.232499</td>\n","      <td>0.230290</td>\n","      <td>0.124551</td>\n","      <td>0.901608</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.184400</td>\n","      <td>0.235681</td>\n","      <td>0.082849</td>\n","      <td>0.235202</td>\n","      <td>0.228399</td>\n","      <td>0.125433</td>\n","      <td>0.901838</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.165000</td>\n","      <td>0.234043</td>\n","      <td>0.081939</td>\n","      <td>0.233532</td>\n","      <td>0.247543</td>\n","      <td>0.132941</td>\n","      <td>0.902216</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0e0e802150845fa997228001d76dff0","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>▇▁█▆▃▄▄</td></tr><tr><td>eval/loss</td><td>█▂▁▂▂▃▂</td></tr><tr><td>eval/mae</td><td>█▂▁▂▂▃▂</td></tr><tr><td>eval/mse</td><td>█▂▂▂▁▁▁</td></tr><tr><td>eval/pearson_corr</td><td>▄▁▆▆▇▇█</td></tr><tr><td>eval/runtime</td><td>▁▇████▇</td></tr><tr><td>eval/samples_per_second</td><td>█▂▁▁▁▁▂</td></tr><tr><td>eval/spearman_corr</td><td>▅▁▆▇▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>█▂▁▁▁▁▂</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▃▆▅▄▆▄█▆▇▇▆▆</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█▆▅▃▁</td></tr><tr><td>train/loss</td><td>██▆▇▅▆▄▄▃▃▂▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>0.90222</td></tr><tr><td>eval/loss</td><td>0.23404</td></tr><tr><td>eval/mae</td><td>0.23353</td></tr><tr><td>eval/mse</td><td>0.08194</td></tr><tr><td>eval/pearson_corr</td><td>0.24754</td></tr><tr><td>eval/runtime</td><td>2.6546</td></tr><tr><td>eval/samples_per_second</td><td>30.514</td></tr><tr><td>eval/spearman_corr</td><td>0.13294</td></tr><tr><td>eval/steps_per_second</td><td>1.13</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/grad_norm</td><td>1.48506</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.165</td></tr><tr><td>train_loss</td><td>0.22346</td></tr><tr><td>train_runtime</td><td>475.9957</td></tr><tr><td>train_samples_per_second</td><td>9.456</td></tr><tr><td>train_steps_per_second</td><td>0.309</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-biencoder-regression-mae</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/irodxcvg' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/irodxcvg</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_011520-irodxcvg/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Train bi-encoder with different loss functions\n","loss_functions = [\"mse\", \"mae\", \"contrastive\", \"cosine_embedding\"]\n","loss_fn = loss_functions[1]\n","wandb.init(project=\"bert-biencoder-regression\", name=f\"bert-biencoder-regression-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","tr = train_biencoder(loss_fn)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:23:41.677891Z","iopub.status.busy":"2024-11-06T01:23:41.677588Z","iopub.status.idle":"2024-11-06T01:25:32.789514Z","shell.execute_reply":"2024-11-06T01:25:32.788460Z","shell.execute_reply.started":"2024-11-06T01:23:41.677857Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to temp_save_bert-reg-biencoder-mae...\n","Saving tokenizer...\n","Pushing to hub at minoosh/bert-reg-biencoder-mae...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:9532: UserWarning: Warnings while validating metadata in README.md:\n","- The pipeline tag \"text-similarity\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, text2text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, any-to-any, other\n","  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e50d8f27903349138976305960be11e2","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Successfully pushed model to minoosh/bert-reg-biencoder-mae\n"]}],"source":["tr.tokenizer = tokenizer\n","repo_id = f\"minoosh/bert-reg-biencoder-{loss_fn}\" \n","save_and_push_to_hub(tr, repo_id)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:25:32.792519Z","iopub.status.busy":"2024-11-06T01:25:32.792100Z","iopub.status.idle":"2024-11-06T01:25:33.967567Z","shell.execute_reply":"2024-11-06T01:25:33.966676Z","shell.execute_reply.started":"2024-11-06T01:25:32.792470Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc42d1c4810e480eb49bf8b7528cb983","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load dataset\n","dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n","\n","# Tokenize both text1 and text2 independently\n","def preprocess_function(examples):\n","    #tokenizer = loaded_tokenizer\n","    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n","    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n","    return {\n","        'input_ids_text1': text1_encodings['input_ids'],\n","        'attention_mask_text1': text1_encodings['attention_mask'],\n","        'input_ids_text2': text2_encodings['input_ids'],\n","        'attention_mask_text2': text2_encodings['attention_mask'],\n","        'labels': examples['label']\n","    }\n","\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","\n","# Remove unnecessary columns and set format for PyTorch\n","columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']\n","tokenized_test.set_format(type='torch', columns=columns_to_keep)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:25:34.308573Z","iopub.status.busy":"2024-11-06T01:25:34.308197Z","iopub.status.idle":"2024-11-06T01:25:38.347424Z","shell.execute_reply":"2024-11-06T01:25:38.346544Z","shell.execute_reply.started":"2024-11-06T01:25:34.308528Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminooshayan97\u001b[0m (\u001b[33mminoosh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_012534-43aui85v</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/uncategorized/runs/43aui85v' target=\"_blank\">noble-sponge-4</a></strong> to <a href='https://wandb.ai/minoosh/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/uncategorized' target=\"_blank\">https://wandb.ai/minoosh/uncategorized</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/uncategorized/runs/43aui85v' target=\"_blank\">https://wandb.ai/minoosh/uncategorized/runs/43aui85v</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["PredictionOutput(predictions=array([0.60868603, 0.76147115, 0.63553685, 0.7573016 , 0.6355754 ,\n","       0.66637826, 0.629447  , 0.5855908 , 0.6867771 , 0.7636793 ,\n","       0.6991831 , 0.65475047, 0.7566673 , 0.6781672 , 0.6747706 ,\n","       0.63560665, 0.59526014, 0.5547283 , 0.63644236, 0.60931593,\n","       0.6662388 , 0.71907336, 0.7167965 , 0.6326246 , 0.75798404,\n","       0.71789443, 0.69856375, 0.64524287, 0.6728792 , 0.7459571 ,\n","       0.76497203, 0.6523702 , 0.47621325, 0.6602509 , 0.727394  ,\n","       0.7407073 , 0.759524  , 0.65505433, 0.6859933 , 0.6314428 ,\n","       0.6089138 , 0.6519295 , 0.62294126, 0.6884976 , 0.73099774,\n","       0.79346174, 0.6113372 , 0.7708464 , 0.6391624 , 0.7504729 ,\n","       0.77726305, 0.7240142 , 0.7362951 , 0.7212137 , 0.70402807,\n","       0.7742696 , 0.6966531 , 0.6064799 , 0.74522364, 0.76917815,\n","       0.7406305 , 0.52029717, 0.70760185, 0.75581133, 0.73019034,\n","       0.6023003 , 0.7116103 , 0.40294296, 0.83097863, 0.6867243 ,\n","       0.5872477 , 0.77481234, 0.6658529 , 0.66007364, 0.673916  ,\n","       0.4229061 , 0.56880057, 0.7664128 , 0.39609656, 0.6263305 ],\n","      dtype=float32), label_ids=array([0.75, 0.5 , 0.5 , 1.  , 0.  , 0.25, 0.5 , 0.5 , 1.  , 0.5 , 0.75,\n","       0.5 , 0.75, 1.  , 0.  , 0.75, 0.5 , 0.5 , 0.25, 0.25, 0.75, 0.25,\n","       0.25, 1.  , 0.5 , 0.5 , 1.  , 0.5 , 0.25, 0.25, 1.  , 1.  , 1.  ,\n","       0.75, 0.75, 0.75, 0.75, 0.5 , 0.75, 0.5 , 0.  , 1.  , 0.  , 0.75,\n","       0.75, 1.  , 0.75, 0.  , 0.75, 1.  , 0.75, 0.75, 0.25, 0.5 , 0.5 ,\n","       0.25, 0.75, 0.75, 0.25, 1.  , 0.25, 0.5 , 0.75, 1.  , 0.5 , 1.  ,\n","       1.  , 0.  , 1.  , 0.75, 0.5 , 1.  , 0.  , 0.75, 0.  , 0.25, 0.75,\n","       0.75, 0.25, 0.5 ], dtype=float32), metrics={'test_loss': 0.24929168820381165, 'test_mse': 0.09943906962871552, 'test_mae': 0.24929165840148926, 'test_pearson_corr': 0.2591931618153467, 'test_spearman_corr': 0.23166598473099176, 'test_cosine_sim': 0.890298068523407, 'test_runtime': 2.6, 'test_samples_per_second': 30.77, 'test_steps_per_second': 1.154})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init()\n","tr.predict(tokenized_test)"]},{"cell_type":"markdown","metadata":{},"source":["# 2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:31:26.206241Z","iopub.status.busy":"2024-11-06T00:31:26.205939Z","iopub.status.idle":"2024-11-06T00:39:18.304853Z","shell.execute_reply":"2024-11-06T00:39:18.303940Z","shell.execute_reply.started":"2024-11-06T00:31:26.206209Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:qnsvrs29) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9b4669ae5794d24b6c57f83feb49034","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">peach-surf-38</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/qnsvrs29' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/qnsvrs29</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_003115-qnsvrs29/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:qnsvrs29). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_003126-mvpniyg5</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/mvpniyg5' target=\"_blank\">bert-biencoder-regression-contrastive</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/mvpniyg5' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/mvpniyg5</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [147/147 07:25, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","      <th>Mae</th>\n","      <th>Pearson Corr</th>\n","      <th>Spearman Corr</th>\n","      <th>Cosine Sim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.215200</td>\n","      <td>0.227630</td>\n","      <td>0.097925</td>\n","      <td>0.238648</td>\n","      <td>0.107580</td>\n","      <td>0.100026</td>\n","      <td>0.904071</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.086200</td>\n","      <td>0.086637</td>\n","      <td>0.114746</td>\n","      <td>0.290732</td>\n","      <td>0.031203</td>\n","      <td>0.050001</td>\n","      <td>0.868783</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.066300</td>\n","      <td>0.078817</td>\n","      <td>0.106616</td>\n","      <td>0.281235</td>\n","      <td>0.103186</td>\n","      <td>0.126869</td>\n","      <td>0.890286</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.056900</td>\n","      <td>0.080645</td>\n","      <td>0.104658</td>\n","      <td>0.281388</td>\n","      <td>0.126067</td>\n","      <td>0.138566</td>\n","      <td>0.889682</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.052400</td>\n","      <td>0.080121</td>\n","      <td>0.111437</td>\n","      <td>0.285503</td>\n","      <td>0.110318</td>\n","      <td>0.103321</td>\n","      <td>0.881939</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.048700</td>\n","      <td>0.080797</td>\n","      <td>0.111748</td>\n","      <td>0.287061</td>\n","      <td>0.118664</td>\n","      <td>0.095060</td>\n","      <td>0.880619</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.045100</td>\n","      <td>0.080986</td>\n","      <td>0.114097</td>\n","      <td>0.291096</td>\n","      <td>0.102437</td>\n","      <td>0.084281</td>\n","      <td>0.877414</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50bdb6d13d8747a79cddbef1ccba2ae3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>█▁▅▅▄▃▃</td></tr><tr><td>eval/loss</td><td>█▁▁▁▁▁▁</td></tr><tr><td>eval/mae</td><td>▁█▇▇▇▇█</td></tr><tr><td>eval/mse</td><td>▁█▅▄▇▇█</td></tr><tr><td>eval/pearson_corr</td><td>▇▁▆█▇▇▆</td></tr><tr><td>eval/runtime</td><td>▁▃▅▇██▇</td></tr><tr><td>eval/samples_per_second</td><td>█▆▄▂▁▁▂</td></tr><tr><td>eval/spearman_corr</td><td>▅▁▇█▅▅▄</td></tr><tr><td>eval/steps_per_second</td><td>█▆▄▂▁▁▂</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆██▅▅▄▂▃▂▂▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▇▄▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>0.87741</td></tr><tr><td>eval/loss</td><td>0.08099</td></tr><tr><td>eval/mae</td><td>0.2911</td></tr><tr><td>eval/mse</td><td>0.1141</td></tr><tr><td>eval/pearson_corr</td><td>0.10244</td></tr><tr><td>eval/runtime</td><td>2.4795</td></tr><tr><td>eval/samples_per_second</td><td>32.668</td></tr><tr><td>eval/spearman_corr</td><td>0.08428</td></tr><tr><td>eval/steps_per_second</td><td>1.21</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/grad_norm</td><td>0.40139</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0451</td></tr><tr><td>train_loss</td><td>0.08568</td></tr><tr><td>train_runtime</td><td>450.1413</td></tr><tr><td>train_samples_per_second</td><td>9.999</td></tr><tr><td>train_steps_per_second</td><td>0.327</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-biencoder-regression-contrastive</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/mvpniyg5' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/mvpniyg5</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_003126-mvpniyg5/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Train bi-encoder with different loss functions\n","loss_functions = [\"mse\", \"mae\", \"contrastive\", \"cosine_embedding\"]\n","loss_fn = loss_functions[2]\n","wandb.init(project=\"bert-biencoder-regression\", name=f\"bert-biencoder-regression-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","tr = train_biencoder(loss_fn)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:39:18.333812Z","iopub.status.busy":"2024-11-06T00:39:18.333456Z","iopub.status.idle":"2024-11-06T00:39:36.122365Z","shell.execute_reply":"2024-11-06T00:39:36.121390Z","shell.execute_reply.started":"2024-11-06T00:39:18.333770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to temp_save_bert-reg-biencoder-contrastive...\n","Saving tokenizer...\n","Pushing to hub at minoosh/bert-reg-biencoder-contrastive...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:9532: UserWarning: Warnings while validating metadata in README.md:\n","- The pipeline tag \"text-similarity\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, text2text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, any-to-any, other\n","  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c4e6ef3fc4b42baa2ad38344b04ce1f","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Successfully pushed model to minoosh/bert-reg-biencoder-contrastive\n"]}],"source":["tr.tokenizer = tokenizer\n","repo_id = f\"minoosh/bert-reg-biencoder-{loss_fn}\" \n","save_and_push_to_hub(tr, repo_id)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:43:07.568879Z","iopub.status.busy":"2024-11-06T00:43:07.568059Z","iopub.status.idle":"2024-11-06T00:43:08.725435Z","shell.execute_reply":"2024-11-06T00:43:08.724351Z","shell.execute_reply.started":"2024-11-06T00:43:07.568834Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd587685c5bc47ef9bd1e5fafe67b079","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load dataset\n","dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n","\n","# Tokenize both text1 and text2 independently\n","def preprocess_function(examples):\n","    #tokenizer = loaded_tokenizer\n","    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n","    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n","    return {\n","        'input_ids_text1': text1_encodings['input_ids'],\n","        'attention_mask_text1': text1_encodings['attention_mask'],\n","        'input_ids_text2': text2_encodings['input_ids'],\n","        'attention_mask_text2': text2_encodings['attention_mask'],\n","        'labels': examples['label']\n","    }\n","\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","\n","# Remove unnecessary columns and set format for PyTorch\n","columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']\n","tokenized_test.set_format(type='torch', columns=columns_to_keep)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:43:53.860275Z","iopub.status.busy":"2024-11-06T00:43:53.859849Z","iopub.status.idle":"2024-11-06T00:43:56.101147Z","shell.execute_reply":"2024-11-06T00:43:56.100086Z","shell.execute_reply.started":"2024-11-06T00:43:53.860238Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["PredictionOutput(predictions=array([0.30174786, 0.3470901 , 0.46558392, 0.33070868, 0.38689995,\n","       0.3594342 , 0.30435553, 0.37352902, 0.28839645, 0.5199138 ,\n","       0.25260442, 0.38173229, 0.3256558 , 0.29137397, 0.31153888,\n","       0.38343948, 0.30422994, 0.28533125, 0.2705326 , 0.27401757,\n","       0.3912194 , 0.52563477, 0.31145102, 0.2694178 , 0.43835   ,\n","       0.45095843, 0.3691133 , 0.24168783, 0.3217611 , 0.30706072,\n","       0.41727334, 0.3716106 , 0.32730258, 0.4408971 , 0.5407703 ,\n","       0.4682087 , 0.48464617, 0.36408383, 0.43082076, 0.3985268 ,\n","       0.36366433, 0.334626  , 0.33188298, 0.43867353, 0.38331282,\n","       0.46235886, 0.16892852, 0.31238768, 0.33027142, 0.29585904,\n","       0.49059904, 0.30969012, 0.50948894, 0.2946757 , 0.3532772 ,\n","       0.3430931 , 0.36803997, 0.2926734 , 0.47204572, 0.50095546,\n","       0.41397455, 0.3904289 , 0.3625942 , 0.58675456, 0.43666878,\n","       0.18831   , 0.40957376, 0.30673474, 0.4662912 , 0.4768079 ,\n","       0.26910836, 0.39610055, 0.35536692, 0.4205851 , 0.46867874,\n","       0.1958513 , 0.26948953, 0.2837917 , 0.09386258, 0.32830447],\n","      dtype=float32), label_ids=array([0.75, 0.5 , 0.5 , 1.  , 0.  , 0.25, 0.5 , 0.5 , 1.  , 0.5 , 0.75,\n","       0.5 , 0.75, 1.  , 0.  , 0.75, 0.5 , 0.5 , 0.25, 0.25, 0.75, 0.25,\n","       0.25, 1.  , 0.5 , 0.5 , 1.  , 0.5 , 0.25, 0.25, 1.  , 1.  , 1.  ,\n","       0.75, 0.75, 0.75, 0.75, 0.5 , 0.75, 0.5 , 0.  , 1.  , 0.  , 0.75,\n","       0.75, 1.  , 0.75, 0.  , 0.75, 1.  , 0.75, 0.75, 0.25, 0.5 , 0.5 ,\n","       0.25, 0.75, 0.75, 0.25, 1.  , 0.25, 0.5 , 0.75, 1.  , 0.5 , 1.  ,\n","       1.  , 0.  , 1.  , 0.75, 0.5 , 1.  , 0.  , 0.75, 0.  , 0.25, 0.75,\n","       0.75, 0.25, 0.5 ], dtype=float32), metrics={'test_loss': 0.07045190036296844, 'test_mse': 0.1506776660680771, 'test_mae': 0.32908040285110474, 'test_pearson_corr': 0.10709377158218558, 'test_spearman_corr': 0.08333282823925263, 'test_cosine_sim': 0.8682577610015869, 'test_runtime': 2.2223, 'test_samples_per_second': 35.999, 'test_steps_per_second': 1.35})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init()\n","tr.predict(tokenized_test)"]},{"cell_type":"markdown","metadata":{},"source":["# 3"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:49:29.611705Z","iopub.status.busy":"2024-11-06T00:49:29.611305Z","iopub.status.idle":"2024-11-06T00:57:31.331432Z","shell.execute_reply":"2024-11-06T00:57:31.330567Z","shell.execute_reply.started":"2024-11-06T00:49:29.611667Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:1fip0ipp) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4d44f2b99714826aed10d3842e69a3a","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">sandy-puddle-40</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/1fip0ipp' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/1fip0ipp</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_004917-1fip0ipp/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:1fip0ipp). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_004929-c0t9xr5w</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/c0t9xr5w' target=\"_blank\">bert-biencoder-regression-cosine_embedding</a></strong> to <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/c0t9xr5w' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/c0t9xr5w</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [147/147 07:34, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","      <th>Mae</th>\n","      <th>Pearson Corr</th>\n","      <th>Spearman Corr</th>\n","      <th>Cosine Sim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.536600</td>\n","      <td>0.521217</td>\n","      <td>0.136733</td>\n","      <td>0.291817</td>\n","      <td>0.107817</td>\n","      <td>0.104922</td>\n","      <td>0.904680</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.529400</td>\n","      <td>0.506096</td>\n","      <td>0.099628</td>\n","      <td>0.239027</td>\n","      <td>0.138539</td>\n","      <td>0.151546</td>\n","      <td>0.904774</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.481100</td>\n","      <td>0.486177</td>\n","      <td>0.085464</td>\n","      <td>0.238144</td>\n","      <td>0.132545</td>\n","      <td>0.089930</td>\n","      <td>0.885686</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.429200</td>\n","      <td>0.449410</td>\n","      <td>0.151136</td>\n","      <td>0.320175</td>\n","      <td>0.215457</td>\n","      <td>0.177835</td>\n","      <td>0.787157</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.367200</td>\n","      <td>0.451259</td>\n","      <td>0.147007</td>\n","      <td>0.306730</td>\n","      <td>0.220642</td>\n","      <td>0.184861</td>\n","      <td>0.797957</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.325900</td>\n","      <td>0.464371</td>\n","      <td>0.220932</td>\n","      <td>0.390079</td>\n","      <td>0.186350</td>\n","      <td>0.166244</td>\n","      <td>0.669631</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.286600</td>\n","      <td>0.460768</td>\n","      <td>0.216300</td>\n","      <td>0.372918</td>\n","      <td>0.181961</td>\n","      <td>0.161866</td>\n","      <td>0.694080</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36d42cfa3dbc4e0b90ce26689c3d7d4e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>██▇▄▅▁▂</td></tr><tr><td>eval/loss</td><td>█▇▅▁▁▂▂</td></tr><tr><td>eval/mae</td><td>▃▁▁▅▄█▇</td></tr><tr><td>eval/mse</td><td>▄▂▁▄▄██</td></tr><tr><td>eval/pearson_corr</td><td>▁▃▃██▆▆</td></tr><tr><td>eval/runtime</td><td>▁█▅▅▇▄▅</td></tr><tr><td>eval/samples_per_second</td><td>█▁▄▄▂▅▄</td></tr><tr><td>eval/spearman_corr</td><td>▂▆▁▇█▇▆</td></tr><tr><td>eval/steps_per_second</td><td>█▁▄▃▂▅▄</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▂▃▄█▃▅▄▃▅▄</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█▆▅▃▁</td></tr><tr><td>train/loss</td><td>▇███▆▆▅▅▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cosine_sim</td><td>0.69408</td></tr><tr><td>eval/loss</td><td>0.46077</td></tr><tr><td>eval/mae</td><td>0.37292</td></tr><tr><td>eval/mse</td><td>0.2163</td></tr><tr><td>eval/pearson_corr</td><td>0.18196</td></tr><tr><td>eval/runtime</td><td>2.4514</td></tr><tr><td>eval/samples_per_second</td><td>33.042</td></tr><tr><td>eval/spearman_corr</td><td>0.16187</td></tr><tr><td>eval/steps_per_second</td><td>1.224</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/grad_norm</td><td>3.78895</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2866</td></tr><tr><td>train_loss</td><td>0.41865</td></tr><tr><td>train_runtime</td><td>459.5377</td></tr><tr><td>train_samples_per_second</td><td>9.795</td></tr><tr><td>train_steps_per_second</td><td>0.32</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-biencoder-regression-cosine_embedding</strong> at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression/runs/c0t9xr5w' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression/runs/c0t9xr5w</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-biencoder-regression' target=\"_blank\">https://wandb.ai/minoosh/bert-biencoder-regression</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241106_004929-c0t9xr5w/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Train bi-encoder with different loss functions\n","loss_functions = [\"mse\", \"mae\", \"contrastive\", \"cosine_embedding\"]\n","loss_fn = loss_functions[3]\n","wandb.init(project=\"bert-biencoder-regression\", name=f\"bert-biencoder-regression-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","tr = train_biencoder(loss_fn)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:57:31.380270Z","iopub.status.busy":"2024-11-06T00:57:31.379961Z","iopub.status.idle":"2024-11-06T00:57:47.444895Z","shell.execute_reply":"2024-11-06T00:57:47.443900Z","shell.execute_reply.started":"2024-11-06T00:57:31.380234Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to temp_save_bert-reg-biencoder-cosine_embedding...\n","Saving tokenizer...\n","Pushing to hub at minoosh/bert-reg-biencoder-cosine_embedding...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:9532: UserWarning: Warnings while validating metadata in README.md:\n","- The pipeline tag \"text-similarity\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, text2text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, any-to-any, other\n","  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d9dbaa7857343b09f7ee8443b24ea01","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Successfully pushed model to minoosh/bert-reg-biencoder-cosine_embedding\n"]}],"source":["tr.tokenizer = tokenizer\n","repo_id = f\"minoosh/bert-reg-biencoder-{loss_fn}\" \n","save_and_push_to_hub(tr, repo_id)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:57:47.446375Z","iopub.status.busy":"2024-11-06T00:57:47.446032Z","iopub.status.idle":"2024-11-06T00:57:48.820995Z","shell.execute_reply":"2024-11-06T00:57:48.819967Z","shell.execute_reply.started":"2024-11-06T00:57:47.446341Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdc4c54db42f4a95b85544b1ec090712","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/80 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load dataset\n","dataset = load_dataset(\"minoosh/Annotated_story_pairs2\")\n","\n","# Tokenize both text1 and text2 independently\n","def preprocess_function(examples):\n","    #tokenizer = loaded_tokenizer\n","    text1_encodings = tokenizer(examples['text1'], truncation=True, padding=True, max_length=512)\n","    text2_encodings = tokenizer(examples['text2'], truncation=True, padding=True, max_length=512)\n","    return {\n","        'input_ids_text1': text1_encodings['input_ids'],\n","        'attention_mask_text1': text1_encodings['attention_mask'],\n","        'input_ids_text2': text2_encodings['input_ids'],\n","        'attention_mask_text2': text2_encodings['attention_mask'],\n","        'labels': examples['label']\n","    }\n","\n","tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","\n","# Remove unnecessary columns and set format for PyTorch\n","columns_to_keep = ['input_ids_text1', 'attention_mask_text1', 'input_ids_text2', 'attention_mask_text2', 'labels']\n","tokenized_test.set_format(type='torch', columns=columns_to_keep)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T00:57:48.824346Z","iopub.status.busy":"2024-11-06T00:57:48.823827Z","iopub.status.idle":"2024-11-06T00:57:52.702731Z","shell.execute_reply":"2024-11-06T00:57:52.701598Z","shell.execute_reply.started":"2024-11-06T00:57:48.824310Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminooshayan97\u001b[0m (\u001b[33mminoosh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_005748-cgezg3wz</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/uncategorized/runs/cgezg3wz' target=\"_blank\">efficient-leaf-2</a></strong> to <a href='https://wandb.ai/minoosh/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/uncategorized' target=\"_blank\">https://wandb.ai/minoosh/uncategorized</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/uncategorized/runs/cgezg3wz' target=\"_blank\">https://wandb.ai/minoosh/uncategorized/runs/cgezg3wz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["PredictionOutput(predictions=array([ 0.26072887,  0.28672993,  0.64897376,  0.84406495,  0.678409  ,\n","        0.18588527,  0.23285288,  0.45452306,  0.07121143,  0.6681566 ,\n","        0.12951618,  0.4418021 ,  0.14077725,  0.5943586 , -0.00660199,\n","        0.12853542,  0.6531828 ,  0.22709721,  0.2340945 ,  0.19466656,\n","        0.4856698 ,  0.52867496,  0.0326823 ,  0.4308136 ,  0.71294254,\n","        0.5404089 ,  0.44032383,  0.02640997,  0.54363096,  0.28043038,\n","        0.39087322,  0.81582737,  0.45609728,  0.5776437 ,  0.4997825 ,\n","        0.63424945,  0.70435804,  0.55719954,  0.39546803,  0.16888575,\n","        0.01424574,  0.09749764, -0.07720552,  0.5839576 ,  0.2537427 ,\n","        0.31187263,  0.54344535,  0.3098923 ,  0.66727346,  0.66106015,\n","        0.7535709 , -0.04317653,  0.44905603,  0.2072299 ,  0.7977514 ,\n","        0.44362992,  0.30361336, -0.1363585 ,  0.40101823,  0.54933596,\n","        0.7151332 , -0.11199361,  0.41664487,  0.80006826,  0.68898124,\n","       -0.05778718,  0.61715233, -0.10330295,  0.84728706,  0.46337813,\n","        0.01195949,  0.04932195,  0.54734015,  0.16432035,  0.6038608 ,\n","        0.08739631, -0.02030603,  0.11518887,  0.01805722,  0.14796618],\n","      dtype=float32), label_ids=array([0.75, 0.5 , 0.5 , 1.  , 0.  , 0.25, 0.5 , 0.5 , 1.  , 0.5 , 0.75,\n","       0.5 , 0.75, 1.  , 0.  , 0.75, 0.5 , 0.5 , 0.25, 0.25, 0.75, 0.25,\n","       0.25, 1.  , 0.5 , 0.5 , 1.  , 0.5 , 0.25, 0.25, 1.  , 1.  , 1.  ,\n","       0.75, 0.75, 0.75, 0.75, 0.5 , 0.75, 0.5 , 0.  , 1.  , 0.  , 0.75,\n","       0.75, 1.  , 0.75, 0.  , 0.75, 1.  , 0.75, 0.75, 0.25, 0.5 , 0.5 ,\n","       0.25, 0.75, 0.75, 0.25, 1.  , 0.25, 0.5 , 0.75, 1.  , 0.5 , 1.  ,\n","       1.  , 0.  , 1.  , 0.75, 0.5 , 1.  , 0.  , 0.75, 0.  , 0.25, 0.75,\n","       0.75, 0.25, 0.5 ], dtype=float32), metrics={'test_loss': 0.47249698638916016, 'test_mse': 0.1854775846004486, 'test_mae': 0.3461190164089203, 'test_pearson_corr': 0.20373171020764858, 'test_spearman_corr': 0.18348148213938, 'test_cosine_sim': 0.7674065232276917, 'test_runtime': 2.3519, 'test_samples_per_second': 34.016, 'test_steps_per_second': 1.276})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init()\n","tr.predict(tokenized_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# save and push to hub"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T01:14:34.671126Z","iopub.status.busy":"2024-11-06T01:14:34.670814Z","iopub.status.idle":"2024-11-06T01:14:39.469519Z","shell.execute_reply":"2024-11-06T01:14:39.468705Z","shell.execute_reply.started":"2024-11-06T01:14:34.671092Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","from huggingface_hub import HfApi\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, BertConfig\n","\n","def save_and_push_to_hub(trainer, repo_id, token=None):\n","    \"\"\"\n","    Save and push Regression BiEncoder model to Hugging Face Hub\n","    \n","    Args:\n","        trainer: Trainer instance containing the model\n","        repo_id: String like 'username/model-name'\n","        token: Optional Hugging Face token\n","    \"\"\"\n","    api = HfApi()\n","    \n","    try:\n","        temp_save_path = f\"temp_save_{repo_id.split('/')[-1]}\"\n","        os.makedirs(temp_save_path, exist_ok=True)\n","        \n","        print(f\"Saving model to {temp_save_path}...\")\n","        \n","        # 1. Save base model configuration\n","        base_config = trainer.model.base_model.config.to_dict()\n","        base_config.update({\n","            \"model_type\": \"bert\",\n","            \"architectures\": [\"BiEncoderModelRegression\"],\n","            \"loss_fn\": trainer.model.loss_fn,\n","            \"task_type\": \"regression\",\n","            \"is_regression\": True\n","        })\n","        \n","        with open(os.path.join(temp_save_path, \"config.json\"), 'w') as f:\n","            json.dump(base_config, f)\n","            \n","        # 2. Save complete model weights\n","        torch.save(trainer.model.state_dict(), os.path.join(temp_save_path, \"pytorch_model.bin\"))\n","        \n","        # 3. Save tokenizer\n","        print(\"Saving tokenizer...\")\n","        tokenizer.save_pretrained(temp_save_path)\n","        \n","        # 4. Save model code\n","        model_code = \"\"\"\n","import torch\n","from transformers import PreTrainedModel\n","\n","class BiEncoderModelRegression(torch.nn.Module):\n","    def __init__(self, base_model, config=None, loss_fn=\"mse\"):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.cos = torch.nn.CosineSimilarity(dim=1)\n","        self.loss_fn = loss_fn\n","        self.config = config\n","\n","    def forward(self, input_ids_text1, attention_mask_text1, input_ids_text2, attention_mask_text2, labels=None):\n","        outputs_text1 = self.base_model(input_ids_text1, attention_mask=attention_mask_text1)\n","        outputs_text2 = self.base_model(input_ids_text2, attention_mask=attention_mask_text2)\n","        \n","        cls_embedding_text1 = outputs_text1.last_hidden_state[:, 0, :]\n","        cls_embedding_text2 = outputs_text2.last_hidden_state[:, 0, :]\n","        \n","        cos_sim = self.cos(cls_embedding_text1, cls_embedding_text2)\n","        \n","        loss = None\n","        if labels is not None:\n","            if self.loss_fn == \"mse\":\n","                loss_fct = torch.nn.MSELoss()\n","            elif self.loss_fn == \"mae\":\n","                loss_fct = torch.nn.L1Loss()\n","            elif self.loss_fn == \"cosine_embedding\":\n","                loss_fct = torch.nn.CosineEmbeddingLoss()\n","                labels_cosine = 2 * (labels > 0.5).float() - 1\n","                return {\"loss\": loss_fct(cls_embedding_text1, cls_embedding_text2, labels_cosine), \"logits\": cos_sim}\n","            \n","            loss = loss_fct(cos_sim, labels)\n","            \n","        return {\"loss\": loss, \"logits\": cos_sim}\n","\"\"\"\n","        with open(os.path.join(temp_save_path, \"modeling.py\"), 'w') as f:\n","            f.write(model_code)\n","        \n","        # 5. Save custom collator\n","        collator_code = \"\"\"\n","import torch\n","\n","class BiEncoderCollator:\n","    def __call__(self, features):\n","        batch = {\n","            'input_ids_text1': torch.stack([f['input_ids_text1'] for f in features]),\n","            'attention_mask_text1': torch.stack([f['attention_mask_text1'] for f in features]),\n","            'input_ids_text2': torch.stack([f['input_ids_text2'] for f in features]),\n","            'attention_mask_text2': torch.stack([f['attention_mask_text2'] for f in features]),\n","            'labels': torch.tensor([f['labels'] for f in features], dtype=torch.float)\n","        }\n","        return batch\n","\"\"\"\n","        with open(os.path.join(temp_save_path, \"data_collator.py\"), 'w') as f:\n","            f.write(collator_code)\n","        \n","        # 6. Create model card\n","        model_card = f\"\"\"---\n","language: en\n","tags:\n","- bert\n","- regression\n","- biencoder\n","- similarity\n","pipeline_tag: text-similarity\n","---\n","\n","# BiEncoder Regression Model\n","\n","This model is a BiEncoder architecture that outputs similarity scores between text pairs.\n","\n","## Model Details\n","- Base Model: bert-base-uncased\n","- Task: Regression\n","- Architecture: BiEncoder with cosine similarity\n","- Loss Function: {trainer.model.loss_fn}\n","\n","## Usage\n","\n","```python\n","from transformers import AutoTokenizer, AutoModel\n","from modeling import BiEncoderModelRegression\n","\n","# Load model components\n","tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n","base_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n","model = BiEncoderModelRegression(base_model, loss_fn=\"{trainer.model.loss_fn}\")\n","\n","# Load weights\n","state_dict = torch.load(\"pytorch_model.bin\")\n","model.load_state_dict(state_dict)\n","\n","# Prepare inputs\n","texts1 = [\"first text\"]\n","texts2 = [\"second text\"]\n","inputs = tokenizer(\n","    texts1, texts2,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",")\n","\n","# Get similarity scores\n","outputs = model(**inputs)\n","similarity_scores = outputs[\"logits\"]\n","```\n","\n","## Metrics\n","The model was trained using {trainer.model.loss_fn} loss and evaluated using:\n","- Mean Squared Error (MSE)\n","- Mean Absolute Error (MAE)\n","- Pearson Correlation\n","- Spearman Correlation\n","- Cosine Similarity\n","\"\"\"\n","        with open(os.path.join(temp_save_path, \"README.md\"), 'w') as f:\n","            f.write(model_card)\n","        \n","        # 7. Push to hub\n","        print(f\"Pushing to hub at {repo_id}...\")\n","        api.upload_folder(\n","            folder_path=temp_save_path,\n","            repo_id=repo_id,\n","            token=token\n","        )\n","        \n","        print(f\"Successfully pushed model to {repo_id}\")\n","        \n","    except Exception as e:\n","        print(f\"Error during push to hub: {str(e)}\")\n","        raise\n","    finally:\n","        if os.path.exists(temp_save_path):\n","            import shutil\n","            shutil.rmtree(temp_save_path)\n","\n","def load_from_hub(repo_id):\n","    \"\"\"\n","    Load regression BiEncoder model from Hugging Face Hub\n","    \"\"\"\n","    try:\n","        print(f\"Loading model from {repo_id}...\")\n","        \n","        # 1. Load configuration and determine loss function\n","        config = AutoConfig.from_pretrained(repo_id)\n","        loss_fn = config.loss_fn if hasattr(config, 'loss_fn') else \"mse\"\n","        \n","        # 2. Initialize base model\n","        base_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n","        \n","        # 3. Create BiEncoder model\n","        model = BiEncoderModel(\n","            base_model=base_model,\n","            config=config,\n","            loss_fn=loss_fn\n","        )\n","        \n","        # 4. Load weights\n","        state_dict = torch.hub.load_state_dict_from_url(\n","            f\"https://huggingface.co/{repo_id}/resolve/main/pytorch_model.bin\",\n","            map_location=\"cpu\"\n","        )\n","        model.load_state_dict(state_dict)\n","        \n","        # 5. Load tokenizer\n","        tokenizer = AutoTokenizer.from_pretrained(repo_id)\n","        \n","        # 6. Create trainer\n","        trainer = Trainer(\n","            model=model,\n","            data_collator=BiEncoderCollator(),\n","            compute_metrics=compute_metrics\n","        )\n","        \n","        print(\"Model loaded successfully!\")\n","        return trainer, model, tokenizer\n","        \n","    except Exception as e:\n","        print(f\"Error loading model from hub: {str(e)}\")\n","        raise"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-11-05T23:43:21.814420Z","iopub.status.busy":"2024-11-05T23:43:21.813548Z","iopub.status.idle":"2024-11-05T23:43:35.079964Z","shell.execute_reply":"2024-11-05T23:43:35.078981Z","shell.execute_reply.started":"2024-11-05T23:43:21.814374Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model from minoosh/rep...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d83c5e53355043739cba547d36fd6162","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://huggingface.co/minoosh/rep/resolve/main/pytorch_model.bin\" to /root/.cache/torch/hub/checkpoints/pytorch_model.bin\n","100%|██████████| 418M/418M [00:10<00:00, 42.3MB/s] \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b9cf011396b4d278a878ad1cb55cbcd","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40aaa076c04447b6aa523bd5aadd76d9","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fc7dc7eb3d84e3aab85327ded7d1f38","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f373367edba047c5857e393aceff21a0","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n"]}],"source":["# Save and push to hub\n","#repo_id = \"minoosh/bert-biencoder-regression\"\n","#save_and_push_to_hub(tr, repo_id)\n","\n","# Load from hub later\n","#loaded_trainer, loaded_model, loaded_tokenizer = load_from_hub(repo_id)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"vscode":{"interpreter":{"hash":"103de50b6741efac643968c186d1b1abfce3e31cc37d5e54d8bda505b68efb83"}}},"nbformat":4,"nbformat_minor":4}
