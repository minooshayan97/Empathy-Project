{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-04T20:00:49.402921Z","iopub.status.busy":"2024-11-04T20:00:49.402625Z","iopub.status.idle":"2024-11-04T20:01:02.813645Z","shell.execute_reply":"2024-11-04T20:01:02.812238Z","shell.execute_reply.started":"2024-11-04T20:00:49.402888Z"},"id":"TY_JJkPo58St","trusted":true},"outputs":[],"source":["!pip install -q transformers datasets wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-11-04T20:01:02.816549Z","iopub.status.busy":"2024-11-04T20:01:02.816136Z","iopub.status.idle":"2024-11-04T20:01:04.472082Z","shell.execute_reply":"2024-11-04T20:01:04.471160Z","shell.execute_reply.started":"2024-11-04T20:01:02.816506Z"},"id":"Um2NZlL_58Su","outputId":"b1856c19-93fd-4bb2-d42b-81fa5df9af2e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token hf_"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"execution":{"iopub.execute_input":"2024-11-04T20:01:04.473970Z","iopub.status.busy":"2024-11-04T20:01:04.473643Z","iopub.status.idle":"2024-11-04T20:05:12.342985Z","shell.execute_reply":"2024-11-04T20:05:12.342233Z","shell.execute_reply.started":"2024-11-04T20:01:04.473934Z"},"id":"XTnEOW4N58Sv","outputId":"b8306239-f2ac-4096-e4be-0e322208d567","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"723bf60f93a44cf2a76fbd66e18ba97d","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112476244444577, max=1.0â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_200504-dh9c8aw3</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/dh9c8aw3' target=\"_blank\">light-planet-100</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/dh9c8aw3' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/dh9c8aw3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87933db8b99a4a90ae7e454ce6de2dfb","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/588 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"766ff2b55e04461f93b96ad1f6c00d30","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/660k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"572e6c12398248b4bcd17b38603efeb3","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/100k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"074df49a505245c8b9544004ab19352a","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/88.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f5465f182d04bd99e9d01314e58322e","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/2467 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f2f31edef264e4a93ddf5be121bb723","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe039abb9dd44f339cbfd21b49bfd21f","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/309 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07d4081e1a094c0bb31f55f1cac3aebb","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"766e48ba644f48049288b82ee706440d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30a8fb35ae3046b1bc07934cf6832ed4","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6755b5b6fb0f402bba5e2d1b0b5b169c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be04e05c50fd4f1093587a2dc2a45228","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2467 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af99dfd5d8bd4e39b42cc273a6ba3d88","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/309 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import wandb\n","\n","# Initialize wandb\n","wandb.init(\n","    project=\"bert-crossencoder-classification\"\n",")\n","\n","# Load dataset\n","dataset = load_dataset(\"minoosh/EPITOME_pairs\")\n","\n","# Initialize the tokenizer and model for cross-encoder setup\n","model_name = \"google-bert/bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Preprocess data for the cross-encoder model by concatenating text1 and text2 with [SEP]\n","def preprocess_function(examples):\n","    # Concatenate both texts with a [SEP] token in between\n","    encodings = tokenizer(examples['text1'], examples['text2'], truncation=True, padding=True, max_length=512)\n","    encodings['labels'] = examples['label']  # Add labels\n","    return encodings\n","\n","# Apply tokenization\n","tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n","#tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n","tokenized_val = dataset['validation'].map(preprocess_function, batched=True)\n","\n","# Set format for PyTorch\n","tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","#tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","\n","\n","# Define compute_metrics function for classification evaluation\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    preds = predictions.argmax(axis=1)\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average=\"weighted\")\n","    recall = recall_score(labels, preds, average=\"weighted\")\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1\n","    }\n","\n","\n","# Custom Cross-Encoder model class for classification\n","class CrossEncoderModel(torch.nn.Module):\n","    def __init__(self, model_name, num_classes=4, loss_fn=\"cross_entropy\"):\n","        super(CrossEncoderModel, self).__init__()\n","        # Load model config\n","        self.config = AutoConfig.from_pretrained(model_name, num_labels=num_classes)\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=self.config)\n","        self.loss_fn = loss_fn\n","\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Output logits for classification\n","\n","        loss = None\n","        if labels is not None:\n","            if self.loss_fn == \"cross_entropy\":\n","                loss_fct = torch.nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n","                loss = loss_fct(logits, labels)\n","            elif self.loss_fn == \"focal_loss\":\n","                # Focal loss implementation for handling class imbalance\n","                alpha = 0.25\n","                gamma = 2.0\n","                ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, labels)\n","                pt = torch.exp(-ce_loss)  # Probability of the true class\n","                loss = (alpha * (1 - pt) ** gamma * ce_loss).mean()\n","            elif self.loss_fn == \"kl_divergence\":\n","                # KL Divergence for soft-label classification\n","                kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","                soft_labels = torch.nn.functional.one_hot(labels, num_classes=self.config.num_labels).float()\n","                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","                loss = kl_div(log_probs, soft_labels)\n","\n","            else:\n","                raise ValueError(f\"Unsupported loss function: {self.loss_fn}\")\n","\n","        return {\"loss\": loss, \"logits\": logits}\n","\n","\n","# Function to initialize and train the cross-encoder model\n","def train_crossencoder(loss_fn):\n","    model = CrossEncoderModel(model_name=model_name, loss_fn=loss_fn)\n","\n","    # Set up TrainingArguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"./output/bert-clf-crossencoder-{loss_fn}\",\n","        evaluation_strategy=\"epoch\",\n","        logging_dir='./logs',\n","        logging_steps=10,\n","        per_device_train_batch_size=wandb.config['batch_size'],\n","        per_device_eval_batch_size=wandb.config['batch_size'],\n","        num_train_epochs=wandb.config['epochs'],\n","        warmup_steps=100,\n","        learning_rate=wandb.config['learning_rate'],\n","        weight_decay=0.01,\n","        report_to=\"wandb\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        push_to_hub=True,\n","        save_total_limit=2\n","    )\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_val,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    # Evaluate the model on the test set\n","    #trainer.evaluate(tokenized_test)\n","    \n","    trainer.model = trainer.model.model\n","\n","    # Save and push the model to the Hugging Face Hub\n","    trainer.save_model(f\"./output/bert-clf-crossencoder-{loss_fn}\")\n","    trainer.push_to_hub(f\"minoosh/bert-clf-crossencoder-{loss_fn}\")\n","\n","    # End the wandb run\n","    wandb.finish()\n","    \n","    return trainer"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-11-04T19:24:18.415082Z","iopub.status.busy":"2024-11-04T19:24:18.414761Z","iopub.status.idle":"2024-11-04T19:39:54.254767Z","shell.execute_reply":"2024-11-04T19:39:54.253899Z","shell.execute_reply.started":"2024-11-04T19:24:18.415048Z"},"id":"DATap4of58Sw","outputId":"0f50d7ce-234a-48ec-95cb-c5f0f4d90f7a","trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:msn6sg38) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbdf3c411b7b47f9a1ad7d8430fcbb7e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">dazzling-donkey-96</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/msn6sg38' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/msn6sg38</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241104_192410-msn6sg38/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:msn6sg38). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_192418-yli264bp</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/yli264bp' target=\"_blank\">bert-crossencoder-classification-kl_divergence</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/yli264bp' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/yli264bp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14543ce7a8214f0a8d225bfebe9aa96c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [546/546 15:02, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.282000</td>\n","      <td>1.217171</td>\n","      <td>0.495146</td>\n","      <td>0.394822</td>\n","      <td>0.495146</td>\n","      <td>0.406126</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.037700</td>\n","      <td>1.024576</td>\n","      <td>0.579288</td>\n","      <td>0.611354</td>\n","      <td>0.579288</td>\n","      <td>0.554970</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.903700</td>\n","      <td>0.943957</td>\n","      <td>0.608414</td>\n","      <td>0.617836</td>\n","      <td>0.608414</td>\n","      <td>0.601548</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.786100</td>\n","      <td>0.938116</td>\n","      <td>0.634304</td>\n","      <td>0.642538</td>\n","      <td>0.634304</td>\n","      <td>0.635553</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.560700</td>\n","      <td>0.971759</td>\n","      <td>0.605178</td>\n","      <td>0.611372</td>\n","      <td>0.605178</td>\n","      <td>0.603379</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.453200</td>\n","      <td>0.967990</td>\n","      <td>0.627832</td>\n","      <td>0.628980</td>\n","      <td>0.627832</td>\n","      <td>0.627540</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.376300</td>\n","      <td>0.991880</td>\n","      <td>0.608414</td>\n","      <td>0.612393</td>\n","      <td>0.608414</td>\n","      <td>0.609859</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb3538d09cee4aaab09f506b2a95fdb8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6ad49919a4e45339bf65670944d3506","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.031 MB of 0.031 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–…â–‡â–ˆâ–‡â–ˆâ–‡</td></tr><tr><td>eval/f1</td><td>â–â–†â–‡â–ˆâ–‡â–ˆâ–‡</td></tr><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–â–â–‚â–‚â–‚</td></tr><tr><td>eval/precision</td><td>â–â–‡â–‡â–ˆâ–‡â–ˆâ–‡</td></tr><tr><td>eval/recall</td><td>â–â–…â–‡â–ˆâ–‡â–ˆâ–‡</td></tr><tr><td>eval/runtime</td><td>â–„â–„â–‚â–ƒâ–â–ƒâ–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–…â–‡â–†â–ˆâ–†â–</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–…â–‡â–†â–ˆâ–†â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ƒâ–†â–ƒâ–â–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–„â–„â–ƒâ–„â–…â–…â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–„â–„â–†â–„â–ˆâ–ƒâ–„â–…â–†â–…â–†â–…â–…â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–‚â–‚â–ƒâ–„â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.60841</td></tr><tr><td>eval/f1</td><td>0.60986</td></tr><tr><td>eval/loss</td><td>0.99188</td></tr><tr><td>eval/precision</td><td>0.61239</td></tr><tr><td>eval/recall</td><td>0.60841</td></tr><tr><td>eval/runtime</td><td>5.2789</td></tr><tr><td>eval/samples_per_second</td><td>58.535</td></tr><tr><td>eval/steps_per_second</td><td>1.894</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>5.29579</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3763</td></tr><tr><td>train_loss</td><td>0.82585</td></tr><tr><td>train_runtime</td><td>905.5883</td></tr><tr><td>train_samples_per_second</td><td>19.069</td></tr><tr><td>train_steps_per_second</td><td>0.603</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-crossencoder-classification-kl_divergence</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/yli264bp' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/yli264bp</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241104_192418-yli264bp/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Specify list of loss functions to try\n","loss_functions = [\"cross_entropy\", \"focal_loss\", \"kl_divergence\"]\n","loss_fn = loss_functions[2] \n","wandb.init(project=\"bert-crossencoder-classification\", name=f\"bert-crossencoder-classification-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","trainer = train_crossencoder(loss_fn)\n","wandb.finish()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T19:42:31.676177Z","iopub.status.busy":"2024-11-04T19:42:31.675629Z","iopub.status.idle":"2024-11-04T19:58:51.588122Z","shell.execute_reply":"2024-11-04T19:58:51.587204Z","shell.execute_reply.started":"2024-11-04T19:42:31.676142Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:jl779bch) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4acb9500d66c4311829b13a24961f196","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">deep-wood-98</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/jl779bch' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/jl779bch</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241104_194223-jl779bch/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:jl779bch). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_194231-r75ozb5g</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/r75ozb5g' target=\"_blank\">bert-crossencoder-classification-focal_loss</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/r75ozb5g' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/r75ozb5g</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c6e54ff73fb4382a61b3c833d9cced5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [546/546 15:45, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.166900</td>\n","      <td>0.158905</td>\n","      <td>0.462783</td>\n","      <td>0.380678</td>\n","      <td>0.462783</td>\n","      <td>0.386787</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.137800</td>\n","      <td>0.139809</td>\n","      <td>0.524272</td>\n","      <td>0.555513</td>\n","      <td>0.524272</td>\n","      <td>0.471277</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.111100</td>\n","      <td>0.125459</td>\n","      <td>0.595469</td>\n","      <td>0.601669</td>\n","      <td>0.595469</td>\n","      <td>0.594746</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.085400</td>\n","      <td>0.124384</td>\n","      <td>0.585761</td>\n","      <td>0.590875</td>\n","      <td>0.585761</td>\n","      <td>0.584022</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.048400</td>\n","      <td>0.131585</td>\n","      <td>0.576052</td>\n","      <td>0.587108</td>\n","      <td>0.576052</td>\n","      <td>0.572238</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.046000</td>\n","      <td>0.137452</td>\n","      <td>0.598706</td>\n","      <td>0.600160</td>\n","      <td>0.598706</td>\n","      <td>0.599190</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.032500</td>\n","      <td>0.141313</td>\n","      <td>0.585761</td>\n","      <td>0.591122</td>\n","      <td>0.585761</td>\n","      <td>0.584792</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2715dcc3dde7463c839f3972e1722050","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1304b502e444c068a7e003e4a06c61e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.031 MB of 0.031 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–„â–ˆâ–‡â–‡â–ˆâ–‡</td></tr><tr><td>eval/f1</td><td>â–â–„â–ˆâ–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–„â–â–â–‚â–„â–„</td></tr><tr><td>eval/precision</td><td>â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/recall</td><td>â–â–„â–ˆâ–‡â–‡â–ˆâ–‡</td></tr><tr><td>eval/runtime</td><td>â–ˆâ–‚â–‚â–â–â–ƒâ–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–â–‡â–‡â–ˆâ–ˆâ–†â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–â–‡â–‡â–ˆâ–ˆâ–†â–‡</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–„â–‚â–…â–‚â–ƒâ–‚â–…â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–„â–‚â–‚â–„â–‚â–‚â–‚â–„â–ƒâ–â–ƒâ–‚â–</td></tr><tr><td>train/learning_rate</td><td>â–‚â–‚â–ƒâ–„â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.58576</td></tr><tr><td>eval/f1</td><td>0.58479</td></tr><tr><td>eval/loss</td><td>0.14131</td></tr><tr><td>eval/precision</td><td>0.59112</td></tr><tr><td>eval/recall</td><td>0.58576</td></tr><tr><td>eval/runtime</td><td>5.5598</td></tr><tr><td>eval/samples_per_second</td><td>55.577</td></tr><tr><td>eval/steps_per_second</td><td>1.799</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>0.50007</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0325</td></tr><tr><td>train_loss</td><td>0.1019</td></tr><tr><td>train_runtime</td><td>948.85</td></tr><tr><td>train_samples_per_second</td><td>18.2</td></tr><tr><td>train_steps_per_second</td><td>0.575</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-crossencoder-classification-focal_loss</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/r75ozb5g' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/r75ozb5g</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241104_194231-r75ozb5g/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Specify list of loss functions to try\n","loss_functions = [\"cross_entropy\", \"focal_loss\", \"kl_divergence\"]\n","loss_fn = loss_functions[1] \n","wandb.init(project=\"bert-crossencoder-classification\", name=f\"bert-crossencoder-classification-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","trainer = train_crossencoder(loss_fn)\n","wandb.finish()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T20:05:12.344396Z","iopub.status.busy":"2024-11-04T20:05:12.344073Z","iopub.status.idle":"2024-11-04T20:20:27.435834Z","shell.execute_reply":"2024-11-04T20:20:27.435133Z","shell.execute_reply.started":"2024-11-04T20:05:12.344364Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:dh9c8aw3) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1afdab4bb3fa4fbcb686adeac8f6d89b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">light-planet-100</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/dh9c8aw3' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/dh9c8aw3</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241104_200504-dh9c8aw3/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:dh9c8aw3). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_200512-renit31v</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/renit31v' target=\"_blank\">bert-crossencoder-classification-cross_entropy</a></strong> to <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/renit31v' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/renit31v</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1ff18c6c4244f148402f6b696a23611","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [546/546 14:35, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.248900</td>\n","      <td>1.206077</td>\n","      <td>0.478964</td>\n","      <td>0.374040</td>\n","      <td>0.478964</td>\n","      <td>0.399903</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.035600</td>\n","      <td>1.023623</td>\n","      <td>0.601942</td>\n","      <td>0.624408</td>\n","      <td>0.601942</td>\n","      <td>0.584063</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.862500</td>\n","      <td>0.998342</td>\n","      <td>0.618123</td>\n","      <td>0.627435</td>\n","      <td>0.618123</td>\n","      <td>0.612645</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.710100</td>\n","      <td>0.968672</td>\n","      <td>0.601942</td>\n","      <td>0.600374</td>\n","      <td>0.601942</td>\n","      <td>0.599811</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.594500</td>\n","      <td>0.996174</td>\n","      <td>0.618123</td>\n","      <td>0.617806</td>\n","      <td>0.618123</td>\n","      <td>0.615657</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.475300</td>\n","      <td>1.024451</td>\n","      <td>0.624595</td>\n","      <td>0.633750</td>\n","      <td>0.624595</td>\n","      <td>0.625634</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.390300</td>\n","      <td>1.040977</td>\n","      <td>0.601942</td>\n","      <td>0.604409</td>\n","      <td>0.601942</td>\n","      <td>0.602915</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99b5d87d29b2416f9ededc7b33a41f36","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c574fd97bb74cc8b3effcbf0a4bbbe6","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡</td></tr><tr><td>eval/f1</td><td>â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡</td></tr><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–‚â–â–‚â–ƒâ–ƒ</td></tr><tr><td>eval/precision</td><td>â–â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡</td></tr><tr><td>eval/recall</td><td>â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡</td></tr><tr><td>eval/runtime</td><td>â–â–ˆâ–…â–„â–â–„â–„</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–â–„â–…â–ˆâ–…â–…</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–â–„â–…â–ˆâ–„â–…</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–‚â–ˆâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚</td></tr><tr><td>train/learning_rate</td><td>â–‚â–‚â–ƒâ–„â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.60194</td></tr><tr><td>eval/f1</td><td>0.60291</td></tr><tr><td>eval/loss</td><td>1.04098</td></tr><tr><td>eval/precision</td><td>0.60441</td></tr><tr><td>eval/recall</td><td>0.60194</td></tr><tr><td>eval/runtime</td><td>5.0235</td></tr><tr><td>eval/samples_per_second</td><td>61.511</td></tr><tr><td>eval/steps_per_second</td><td>1.991</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>6.71954</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3903</td></tr><tr><td>train_loss</td><td>0.79411</td></tr><tr><td>train_runtime</td><td>879.0495</td></tr><tr><td>train_samples_per_second</td><td>19.645</td></tr><tr><td>train_steps_per_second</td><td>0.621</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">bert-crossencoder-classification-cross_entropy</strong> at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification/runs/renit31v' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification/runs/renit31v</a><br/> View project at: <a href='https://wandb.ai/minoosh/bert-crossencoder-classification' target=\"_blank\">https://wandb.ai/minoosh/bert-crossencoder-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241104_200512-renit31v/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Specify list of loss functions to try\n","loss_functions = [\"cross_entropy\", \"focal_loss\", \"kl_divergence\"]\n","loss_fn = loss_functions[0] \n","wandb.init(project=\"bert-crossencoder-classification\", name=f\"bert-crossencoder-classification-{loss_fn}\", config={\"epochs\": 7, \"batch_size\": 16, \"learning_rate\": 2e-5})\n","trainer = train_crossencoder(loss_fn)\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
